{
  "hash": "ffc9b324fe258b2cfa7be46bad6c46cb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Estimating the Unobserved: A Simulation Study on Censoring and Truncation in Race Models of Choice and Response Time\"\nshorttitle: \"Race Model Censoring and Truncation\"\nauthor:\n  - name: Jeroen E. Timmerman\n    corresponding: true\n    orcid: 0009-0003-8208-0509\n    email: j.e.timmerman@uva.nl\n    affiliations:\n      - name: University of Amsterdam\n        department: Department of Psychology\n        address: Nieuwe Achtergracht 129-B\n        city: Amsterdam\n        region: North-Holland\n        postal-code: 1018 WS\nabstract: \"Race models of decision making, such as the linear ballistic accumulator (LBA), log-normal race (LNR) and the racing diffusion model (RDM) are widely used to model response times (RTs) and choices as a race between choice options. These models assume the shapes of the RT distributions to reflect latent parameters, such as a participant’s tendency to emphasize accuracy over speed. Since RT data often does not reflect the full RT distribution due to missing data from trial timeouts, outlier removal, or other limitations,  which affects the parameter recovery for these models. While missing data is commonly handled by truncation—excluding the missing trials from the analysis—it can also be handled by censoring, which takes the proportion of missing trials into account. In two simulation studies, the parameter recovery for these methods are compared for the LBA, LNR, and RDM. In the first simulation study, we showed how upper truncation biases parameter estimation for all models, while censoring had good parameter identifiability with the large sample size used. In a second simulation with a smaller number of trials, censoring had better parameter recovery in some cases, particularly when the lower tail was missing, while in other cases the parameter recovery of censoring and truncation were not notably different. These results suggest that censoring should be preferred over truncation when the objective is accurate parameter estimation, but since censoring is computationally costly, truncation can be admissible in some analyses with a small number of trials.\"\nkeywords: [censoring, truncation, missing data, evidence accumulation, choice response data, diffusion decision model, linear ballistic accumulator, Bayesian hierarchical modeling. decision making]\nauthor-note:\n  disclosures:\n    conflict of interest: The author has no conflict of interest to declare.\ndocumentmode: stu\ncourse: \"MI2324RM: Research Master's Internship\"\nprofessor: Dr. Andrew J. Heathcote\nduedate: \"June 13th, 2025\"\nformat:\n  apaquarto-html:\n    mainfont: Helvetica, sans-serif\n    anchor-sections: true\n    title-block-banner: true\n  apaquarto-pdf: default\nbibliography: references.bib\nnotebook-links: false\nlicense: \"CC BY-NC\"\nexecute:\n  cache: true\ndraftfirst: true\n---\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\nlibrary(EMC2)\nlibrary(rtdists)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(BayesFactor)\nlibrary(ggh4x)\nlibrary(kableExtra)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nft_colors <- c(\"#C400C4\", \"#0EBF0E\") # for false / true, correct / incorrect combinations\nct_colors <- c(\"#FF8700\", \"#0C7BDC\") # For censoring / truncation\n```\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\nparsLBA <- c(log(2),3,1,log(2),log(.75),log(.2))\nnames(parsLBA) <- c(\"B\", \"v\", \"v_lMd\", \"A\", \"sv_lMd\", \"t0\")\nparsLBA_label <- c(\"B\", \"v\", \"v[true]\", \"A\", \"s[v[true]]\", \"t[0]\") # for ggplot\nparsRDM <- c(log(3),1,.4,log(.75),log(.2))\nnames(parsRDM) <- c(\"B\", \"v\", \"v_lMd\", \"s_lMd\", \"t0\")\nparsRDM_label <- c(\"B\", \"v\", \"v[true]\", \"s[true]\", \"t0\")\nparsLNR <- c(log(.75),log(.65),log(.5),log(.8),log(.4))\nnames(parsLNR) <- c(\"m\", \"m_lMd\", \"s\", \"s_lMd\", \"t0\")\nparsLNR_label <- c(\"m\", \"m[true]\", \"s\", \"s[true]\", \"t0\")\n\nparlabeller <- c(parsLBA_label, parsRDM_label, parsLNR_label)\nnames(parlabeller) <- c(names(parsLBA), names(parsRDM), names(parsLNR))\nparlabeller <- parlabeller[!duplicated(parlabeller)]\n```\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\nfile_names <- list.files(\"01_simulation/upper_censoring/EMCs\")\nEMCs_upper <- data.frame()\nfor (i in file_names){\n  load(paste0(\"01_simulation/upper_censoring/EMCs/\", i))\n  EMC <- data.frame(EMC = str_remove(i, \".RData\"),\n                    Censoring = str_detect(i, \"sM\"),\n                    Model = str_extract(i, \"LBA|LNR|RDM\"),\n                    Missing = str_extract(i, \"70|90|975\")\n                    )\n  EMCs_upper <- rbind(EMCs_upper, EMC)\n}\nEMCs_upper <- na.omit(EMCs_upper) # to remove empty sLBA\nEMCs_upper$Censoring <- factor(EMCs_upper$Censoring, c(FALSE, TRUE), c(\"Truncated\", \"Censored\"))\nEMCs_upper$Model <- factor(EMCs_upper$Model)\nEMCs_upper$Missing <- factor(EMCs_upper$Missing, c(975, 90, 70), c(\"2.5%\", \"10%\", \"30%\"))\n```\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\npars_upper <- list()\nfor (i in 1:nrow(EMCs_upper)) {\n  pars_upper[[EMCs_upper$EMC[i]]] <-\n    as.data.frame(summary(get(EMCs_upper$EMC[i], envir = globalenv()),\n                          selection = \"alpha\")[[1]])\n  EMCs_upper$RMSE[i] <- sqrt(mean((pars_upper[[EMCs_upper$EMC[i]]][,2] - get(paste0(\"pars\", EMCs_upper$Model[i]), envir = globalenv()))^2))\n  EMCs_upper$MAE[i] <- mean(abs(pars_upper[[EMCs_upper$EMC[i]]][,2] - get(paste0(\"pars\", EMCs_upper$Model[i]))))\n  EMCs_upper$R[i] <- cor(pars_upper[[EMCs_upper$EMC[i]]][,2], get(paste0(\"pars\", EMCs_upper$Model[i])))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n alpha Jeroen \n         2.5%    50%  97.5%  Rhat  ESS\nB       1.683  1.715  1.748 1.002 2010\nv       7.588  7.871  8.162 1.002 2029\nv_lMd   1.366  1.441  1.520 1.002 2227\nA       1.273  1.324  1.373 1.002 2129\nsv_lMd -0.593 -0.537 -0.479 1.001 2338\nt0     -2.995 -2.977 -2.899 1.001 2214\n\n alpha Jeroen \n         2.5%    50%  97.5%  Rhat  ESS\nB       1.577  1.613  1.647 1.001 2245\nv       6.703  6.975  7.248 1.001 2254\nv_lMd   1.412  1.480  1.549 1.000 2233\nA       1.495  1.548  1.597 1.002 2157\nsv_lMd -0.620 -0.566 -0.510 1.001 2467\nt0     -2.995 -2.968 -2.855 1.001 2231\n\n alpha Jeroen \n         2.5%    50%  97.5%  Rhat  ESS\nB       1.108  1.214  1.312 1.000 2390\nv       4.536  4.804  5.072 1.000 2361\nv_lMd   1.122  1.171  1.222 1.004 2090\nA       1.198  1.266  1.334 1.001 2271\nsv_lMd -0.418 -0.374 -0.330 1.001 2172\nt0     -2.772 -2.315 -2.001 1.000 2334\n\n alpha Jeroen \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.105 -0.099 -0.095 1.000 2213\nm_lMd -0.174 -0.167 -0.160 1.000 2312\ns     -1.681 -1.667 -1.652 1.001 2291\ns_lMd -0.265 -0.238 -0.210 1.002 2507\nt0    -2.995 -2.969 -2.867 1.000 2220\n\n alpha Jeroen \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.011  0.007  0.013 1.003 2174\nm_lMd -0.222 -0.214 -0.206 1.001 2200\ns     -1.434 -1.419 -1.398 1.004 2195\ns_lMd -0.259 -0.235 -0.212 1.002 2375\nt0    -2.992 -2.907 -2.630 1.003 2149\n\n alpha Jeroen \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.209 -0.179 -0.145 1.002 2479\nm_lMd -0.353 -0.336 -0.317 1.002 2316\ns     -1.008 -0.963 -0.921 1.002 2335\ns_lMd -0.268 -0.244 -0.222 1.001 2219\nt0    -1.360 -1.255 -1.175 1.002 2483\n\n alpha Jeroen \n         2.5%    50%  97.5%  Rhat  ESS\nB       0.579  0.731  0.917 1.004 2238\nv       2.854  3.224  3.727 1.004 2198\nv_lMd   0.987  1.035  1.091 1.001 2284\nA       0.644  0.809  0.983 1.003 2152\nsv_lMd -0.357 -0.313 -0.272 1.003 2202\nt0     -1.884 -1.618 -1.441 1.003 2564\n\n alpha Jeroen \n         2.5%    50%  97.5%  Rhat  ESS\nB       0.479  0.577  0.683 1.002 2214\nv       2.678  2.839  3.005 1.001 2282\nv_lMd   0.973  1.013  1.055 1.001 2291\nA       0.568  0.657  0.745 1.000 2310\nsv_lMd -0.317 -0.281 -0.244 1.001 2397\nt0     -1.589 -1.441 -1.325 1.002 2231\n\n alpha Jeroen \n         2.5%    50%  97.5%  Rhat  ESS\nB       0.469  0.562  0.653 1.000 2209\nv       2.725  2.844  2.974 1.002 2873\nv_lMd   0.971  1.007  1.049 1.001 2266\nA       0.599  0.666  0.734 1.001 2466\nsv_lMd -0.315 -0.278 -0.242 1.001 2037\nt0     -1.557 -1.424 -1.316 1.000 2248\n\n alpha Jeroen \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.301 -0.279 -0.255 1.001 2435\nm_lMd -0.441 -0.416 -0.393 1.002 2195\ns     -0.769 -0.721 -0.676 1.001 2225\ns_lMd -0.247 -0.225 -0.204 1.000 2111\nt0    -0.993 -0.941 -0.896 1.001 2361\n\n alpha Jeroen \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.296 -0.276 -0.253 1.001 2309\nm_lMd -0.451 -0.430 -0.410 1.002 2354\ns     -0.751 -0.714 -0.681 1.002 2185\ns_lMd -0.260 -0.238 -0.216 1.001 2315\nt0    -0.983 -0.937 -0.902 1.002 2243\n\n alpha Jeroen \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.302 -0.282 -0.261 1.001 2397\nm_lMd -0.461 -0.442 -0.422 1.000 2352\ns     -0.716 -0.686 -0.656 1.000 2701\ns_lMd -0.268 -0.245 -0.223 1.000 2155\nt0    -0.954 -0.917 -0.885 1.001 2388\n\n alpha Jeroen \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.257  1.272  1.288 1.001 2387\nv      1.108  1.123  1.138 1.003 2156\nv_lMd  0.326  0.340  0.354 1.002 2242\ns_lMd -0.303 -0.283 -0.262 1.001 2225\nt0    -2.303 -2.190 -2.101 1.000 2300\n\n alpha Jeroen \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.265  1.279  1.294 1.000 2652\nv      1.096  1.110  1.125 1.000 2309\nv_lMd  0.326  0.340  0.354 1.000 2308\ns_lMd -0.308 -0.288 -0.267 1.000 2355\nt0    -2.424 -2.311 -2.213 1.001 2691\n\n alpha Jeroen \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.249  1.264  1.278 1.002 2250\nv      1.070  1.085  1.100 1.001 2487\nv_lMd  0.348  0.362  0.378 1.000 2678\ns_lMd -0.338 -0.318 -0.298 1.002 2445\nt0    -2.406 -2.303 -2.206 1.001 2213\n\n alpha Jeroen \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.628  1.637  1.645 1.001 2349\nv      1.629  1.637  1.645 1.001 2200\nv_lMd  0.176  0.183  0.189 1.002 2312\ns_lMd -0.347 -0.332 -0.316 1.002 2413\nt0    -2.995 -2.965 -2.858 1.000 1976\n\n alpha Jeroen \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.566  1.572  1.579 1.011 1439\nv      1.486  1.493  1.499 1.008 1382\nv_lMd  0.174  0.180  0.186 1.008 1332\ns_lMd -0.142 -0.130 -0.117 1.008 1369\nt0    -2.995 -2.988 -2.959 1.004 1344\n\n alpha Jeroen \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.234  1.273  1.333 1.000 2275\nv      1.121  1.145  1.179 1.000 2128\nv_lMd  0.322  0.343  0.362 1.001 2099\ns_lMd -0.333 -0.312 -0.292 0.999 2352\nt0    -2.468 -2.107 -1.934 1.000 2233\n```\n\n\n:::\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\nparameters_CI_upper <- do.call(rbind, pars_upper)\nfor (i in 1:nrow(parameters_CI_upper)) {\n  name <- row.names(parameters_CI_upper)[i]\n  parameters_CI_upper$Censoring[i] <- str_detect(name, \"sM\")\n  parameters_CI_upper$Model[i] <- str_extract(name, \"LBA|LNR|RDM\")\n  parameters_CI_upper$Missing[i] <- str_extract(name, \"70|90|975\")\n  parameters_CI_upper$Parameter[i] <- sub(\".*\\\\.\", \"\", name)\n  parameters_CI_upper$True[i] <- \n    get(paste0(\"pars\", parameters_CI_upper$Model[i]), envir = globalenv())[parameters_CI_upper$Parameter[i]]\n}\n\nparameters_CI_upper$Missing <- factor(parameters_CI_upper$Missing, c(975, 90, 70), c(\"2.5%\", \"10%\", \"30%\"))\nparameters_CI_upper$Censoring <- factor(parameters_CI_upper$Censoring, c(TRUE, FALSE), c(\"Censored\", \"Truncated\"))\n```\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\nWasserstein_upper <- data.frame()\n\nfor (i in 1:nrow(EMCs_upper)) {\n  MCMC_sample <- get_pars(get(EMCs_upper$EMC[i], envir = globalenv()),\n                          return_mcmc = F, merge_chains = T)\n  true_pars <- get(paste0(\"pars\", as.character(EMCs_upper$Model[i])))\n  W <- c(model = sqrt(mean(sqrt(colSums((MCMC_sample[,1,] - true_pars)^2)^2))),\n         sqrt(apply((MCMC_sample[,1,] - true_pars)^2, 1, mean)))\n  W_column <- data.frame(W = W, Variable = names(W))\n  W_addition <- cbind(EMCs_upper[i,], W_column,\n                                     row.names = NULL)\n  Wasserstein_upper <- rbind(Wasserstein_upper, W_addition)\n}\n```\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\nfile_names_both <- list.files(\"01_simulation/censoring_both/EMCs\")\nEMCs_both <- data.frame()\nfor (i in file_names_both){\n  load(paste0(\"01_simulation/censoring_both/EMCs/\", i))\n  EMC <- data.frame(EMC = str_remove(i, \".RData\"),\n                    Censoring = str_detect(i, \"sM\"),\n                    Model = str_extract(i, \"LBA|LNR|RDM\"),\n                    Tail = str_extract(i, \"lower|upper|both\"),\n                    Missing = str_extract(i, \"2|10|30|50\"),\n                    Response = !str_detect(i, \"unknown\")\n                    )\n  assign(EMC$EMC, s)\n  EMCs_both <- rbind(EMCs_both, EMC)\n}\n\nEMCs_both$Censoring <- factor(EMCs_both$Censoring, c(FALSE, TRUE),\n                         c(\"Truncated\", \"Censored\"))\nEMCs_both$Model <- factor(EMCs_both$Model)\nEMCs_both$Response <- factor(EMCs_both$Response, c(FALSE, TRUE),\n                              c(\"unknown\", \"known\"))\n```\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\npars_EMCs_both <- data.frame()\nfor (i in 1:nrow(EMCs_both)) {\n  summ <- do.call(rbind, summary(get(EMCs_both$EMC[i]), by_subject = T))\n  pars_EMCs_both <-\n    rbind(\n      pars_EMCs_both,\n      data.frame(\n        EMC = EMCs_both$EMC[i],\n        Model = EMCs_both$Model[i],\n        Censoring = EMCs_both$Censoring[i],\n        Tail = EMCs_both$Tail[i],\n        Missing = EMCs_both$Missing[i],\n        Response = EMCs_both$Response[i],\n        id = rep(1:10, each = nrow(summ) / 10),\n        Variable = row.names(summ),\n        True = get(paste0(\"pars\", EMCs_both$Model[i]))[row.names(summ)],\n        Q2.5 = summ[, 1],\n        Median = summ[, 2],\n        Q97.5 = summ[, 3],\n        Rhat = summ[, 4],\n        ESS = summ[, 5]\n      )\n    )\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -2.127 -0.999 -0.109 1.003 2046\nv       1.575  2.708  3.740 1.002 2243\nv_lMd   1.546  2.043  2.764 1.003 2262\nA       0.483  0.806  1.098 1.000 2358\nsv_lMd -1.076 -0.498  0.011 1.001 2216\nt0     -0.653 -0.474 -0.382 1.005 1984\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -1.530 -0.738 -0.099 1.002 2392\nv       1.934  2.897  3.978 1.002 2416\nv_lMd   1.067  1.447  1.979 1.001 2405\nA       0.480  0.831  1.180 1.002 2176\nsv_lMd -0.972 -0.392  0.121 1.000 2350\nt0     -0.667 -0.521 -0.436 1.001 2242\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -0.444  0.221  0.882 1.000 2051\nv       1.585  2.375  3.186 1.000 2206\nv_lMd   0.784  1.104  1.550 1.001 2352\nA      -0.269  0.452  0.866 1.000 2269\nsv_lMd -0.755 -0.416 -0.143 1.001 2219\nt0     -1.862 -1.032 -0.730 1.001 1953\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -1.020 -0.301  0.351 1.001 2272\nv       1.387  2.355  3.330 1.001 2330\nv_lMd   1.226  1.684  2.388 1.001 2165\nA       0.535  0.815  1.090 1.000 2386\nsv_lMd -1.184 -0.763 -0.351 1.000 2169\nt0     -1.086 -0.761 -0.597 1.001 2282\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -1.699 -0.945 -0.148 1.003 2094\nv       2.088  3.357  4.558 1.000 2204\nv_lMd   1.223  1.818  2.664 1.001 2183\nA       0.396  0.722  1.016 1.002 2353\nsv_lMd -1.639 -0.942 -0.288 1.000 2314\nt0     -0.468 -0.350 -0.295 1.004 2245\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -1.697 -0.861 -0.167 1.001 2266\nv       1.883  3.094  4.303 1.002 2248\nv_lMd   1.048  1.598  2.475 1.001 2433\nA       0.236  0.558  0.858 1.003 2266\nsv_lMd -1.658 -1.072 -0.466 1.000 2485\nt0     -0.477 -0.364 -0.299 1.000 2112\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -1.827 -1.047 -0.341 1.001 2387\nv       1.793  3.008  4.307 1.001 2372\nv_lMd   1.625  2.427  3.566 1.001 2215\nA      -0.037  0.292  0.596 1.002 2299\nsv_lMd -1.940 -1.328 -0.702 1.002 2254\nt0     -0.348 -0.265 -0.218 1.001 2444\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -2.100 -1.232 -0.509 1.002 2124\nv       1.796  3.140  4.425 1.003 2110\nv_lMd   1.470  2.179  3.258 1.000 1784\nA      -0.088  0.262  0.563 1.001 2226\nsv_lMd -1.772 -1.045 -0.272 1.000 2064\nt0     -0.320 -0.249 -0.208 1.003 2210\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -2.522 -1.599 -0.776 1.000 2529\nv       1.116  2.040  2.890 1.004 2312\nv_lMd   1.115  1.516  2.015 1.001 2169\nA       0.217  0.620  0.996 1.003 2302\nsv_lMd -0.417  0.238  0.946 1.000 2151\nt0     -0.466 -0.357 -0.307 1.000 2293\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -2.709 -1.573 -0.704 1.003 2347\nv       0.422  1.350  2.133 1.000 2166\nv_lMd   1.081  1.594  2.582 1.000 2252\nA      -0.107  0.254  0.598 1.000 1971\nsv_lMd -0.970 -0.495  0.013 1.000 2257\nt0     -0.522 -0.386 -0.316 1.003 2222\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -0.978 -0.357  0.261 1.001 2426\nv       1.697  2.512  3.329 1.003 2572\nv_lMd   0.953  1.287  1.758 1.000 2586\nA       0.507  0.827  1.123 1.004 2635\nsv_lMd -0.843 -0.418 -0.015 1.001 2587\nt0     -0.956 -0.686 -0.550 1.001 2426\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -1.420 -0.635  0.114 1.003 2174\nv       1.369  2.090  2.784 1.004 2353\nv_lMd   0.712  1.021  1.451 1.001 2076\nA       0.133  0.516  0.840 1.001 2102\nsv_lMd -0.631 -0.228  0.119 1.000 2245\nt0     -0.952 -0.651 -0.517 1.002 2121\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -3.500 -2.174 -1.059 1.002 1916\nv       0.392  1.310  2.251 1.000 2154\nv_lMd   0.749  1.289  2.264 1.002 2084\nA      -0.265  0.185  0.634 1.001 2263\nsv_lMd -0.988 -0.403  0.283 1.000 2143\nt0     -0.286 -0.185 -0.146 1.004 2018\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -3.096 -2.191 -1.355 1.000 2181\nv       0.301  1.229  2.149 1.001 2284\nv_lMd   0.789  1.308  2.301 1.000 2700\nA      -0.438 -0.021  0.411 1.002 2364\nsv_lMd -0.936 -0.385  0.234 1.001 2280\nt0     -0.240 -0.179 -0.151 1.001 2237\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -5.203 -3.897 -2.657 1.001 2210\nv      -0.088  0.953  2.003 1.001 2293\nv_lMd   0.484  1.450  2.450 1.001 2224\nA      -0.730 -0.186  0.368 1.000 2356\nsv_lMd -0.525  0.304  1.561 1.001 2246\nt0     -0.049 -0.020 -0.008 1.001 2176\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -4.150 -3.177 -2.074 1.001 2232\nv       0.070  1.065  2.035 1.004 2071\nv_lMd   0.510  1.452  2.347 1.000 2570\nA      -0.704 -0.161  0.372 1.003 2055\nsv_lMd -0.436  0.365  1.685 1.003 2279\nt0     -0.078 -0.038 -0.025 1.002 2141\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB       0.262  0.767  1.165 1.001 2222\nv       2.886  3.732  4.684 1.000 1999\nv_lMd   0.830  1.113  1.467 1.002 2311\nA       0.314  0.752  1.073 1.001 2130\nsv_lMd -0.684 -0.375 -0.080 1.001 2317\nt0     -2.258 -1.411 -0.991 1.002 2216\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -0.065  0.511  1.027 1.000 2275\nv       2.949  3.882  4.983 1.000 2327\nv_lMd   0.607  0.925  1.289 1.001 2477\nA       0.605  0.957  1.259 1.000 2411\nsv_lMd -0.858 -0.462 -0.127 1.000 1986\nt0     -1.612 -1.046 -0.776 1.001 2181\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB       0.028  0.556  1.004 1.001 2532\nv       2.035  2.835  3.671 1.000 2216\nv_lMd   0.836  1.112  1.477 1.002 2473\nA      -0.045  0.639  1.034 1.000 2560\nsv_lMd -0.686 -0.376 -0.104 1.002 2207\nt0     -2.390 -1.404 -0.983 1.002 2277\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB       0.041  0.509  0.930 1.000 2148\nv       2.480  3.228  4.120 1.001 2131\nv_lMd   0.846  1.137  1.460 1.000 2329\nA       0.469  0.858  1.160 1.002 2292\nsv_lMd -0.641 -0.314  0.008 1.000 2121\nt0     -1.897 -1.242 -0.928 1.001 2264\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -0.362  0.186  0.736 1.000 1943\nv       2.131  3.477  4.810 1.000 2169\nv_lMd   1.086  1.803  2.930 1.001 2294\nA       0.370  0.694  0.986 1.001 2255\nsv_lMd -2.108 -1.467 -0.918 1.000 2199\nt0     -1.174 -0.850 -0.686 1.001 1828\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB       0.546  0.996  1.359 1.004 2028\nv       3.643  4.543  5.537 1.002 2169\nv_lMd   0.716  1.027  1.398 1.003 2067\nA      -0.093  0.571  0.925 1.005 2462\nsv_lMd -0.267  0.046  0.331 1.001 2289\nt0     -2.413 -1.546 -1.107 1.002 2034\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB       0.057  0.744  1.225 1.004 2088\nv       2.802  3.971  5.152 1.005 2208\nv_lMd   0.835  1.262  1.894 1.002 2270\nA      -0.090  0.378  0.722 0.999 2249\nsv_lMd -1.160 -0.719 -0.349 1.002 2197\nt0     -2.310 -1.418 -0.942 1.001 1911\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB       0.174  0.785  1.258 1.000 2070\nv       3.032  4.152  5.288 1.000 2190\nv_lMd   0.681  1.080  1.597 1.002 2102\nA      -0.661  0.253  0.644 1.000 2680\nsv_lMd -0.901 -0.426 -0.047 1.002 2184\nt0     -2.189 -1.364 -0.957 1.000 2115\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.611 -0.418 -0.167 1.001 2648\nm_lMd -0.548 -0.371 -0.241 1.002 2369\ns     -1.085 -0.756 -0.458 1.001 2565\ns_lMd -0.375 -0.205 -0.035 1.002 2353\nt0    -1.291 -0.776 -0.562 1.002 2440\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.834 -0.701 -0.529 1.002 2217\nm_lMd -0.626 -0.454 -0.308 1.000 2232\ns     -0.735 -0.485 -0.288 1.000 2175\ns_lMd -0.279 -0.118  0.052 1.001 2303\nt0    -0.712 -0.546 -0.468 1.001 2231\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.517 -0.390 -0.234 1.000 2272\nm_lMd -0.659 -0.490 -0.352 1.002 2222\ns     -0.849 -0.620 -0.432 1.000 2284\ns_lMd -0.398 -0.221 -0.066 1.000 2564\nt0    -1.035 -0.774 -0.639 1.001 2155\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.574 -0.454 -0.296 1.003 2452\nm_lMd -0.533 -0.387 -0.261 1.001 2450\ns     -0.843 -0.617 -0.444 1.002 2365\ns_lMd -0.378 -0.218 -0.056 1.001 2234\nt0    -0.959 -0.721 -0.605 1.001 2407\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.940 -0.672 -0.337 1.003 2137\nm_lMd -0.500 -0.314 -0.185 1.003 2291\ns     -1.411 -0.993 -0.601 1.002 2166\ns_lMd -0.407 -0.208 -0.019 1.001 2338\nt0    -1.067 -0.606 -0.416 1.006 2341\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.894 -0.591 -0.213 1.001 2207\nm_lMd -0.402 -0.238 -0.134 1.002 2258\ns     -1.523 -1.070 -0.651 1.001 2139\ns_lMd -0.342 -0.156  0.034 1.001 2270\nt0    -1.376 -0.692 -0.445 1.001 2233\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -1.265 -0.862 -0.311 1.000 2271\nm_lMd -0.580 -0.302 -0.142 0.999 2086\ns     -1.823 -1.154 -0.565 1.000 2264\ns_lMd -0.562 -0.302 -0.072 1.000 2164\nt0    -1.215 -0.505 -0.299 1.000 2402\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -1.253 -0.835 -0.291 1.000 2084\nm_lMd -0.430 -0.227 -0.111 1.001 2173\ns     -1.951 -1.310 -0.757 1.000 2128\ns_lMd -0.384 -0.141  0.081 1.003 2175\nt0    -1.305 -0.544 -0.325 1.000 2184\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.952 -0.837 -0.720 1.001 2383\nm_lMd -0.988 -0.740 -0.541 1.004 2320\ns     -0.231 -0.074  0.067 1.006 2207\ns_lMd -0.437 -0.262 -0.103 1.006 2290\nt0    -0.420 -0.377 -0.352 1.003 2219\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.736 -0.619 -0.493 1.000 2346\nm_lMd -1.088 -0.818 -0.596 0.999 2327\ns     -0.303 -0.131  0.034 1.001 2609\ns_lMd -0.570 -0.386 -0.213 1.002 2194\nt0    -0.478 -0.416 -0.376 1.002 2607\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.606 -0.489 -0.349 1.000 2510\nm_lMd -0.688 -0.529 -0.391 1.001 2236\ns     -0.739 -0.539 -0.370 1.000 2419\ns_lMd -0.317 -0.149  0.003 1.001 2058\nt0    -0.817 -0.641 -0.551 1.000 2672\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.616 -0.496 -0.354 1.001 2417\nm_lMd -0.614 -0.454 -0.324 1.003 2570\ns     -0.710 -0.504 -0.329 1.003 2645\ns_lMd -0.318 -0.157 -0.001 1.001 2242\nt0    -0.806 -0.635 -0.543 1.003 2705\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -1.007 -0.875 -0.733 1.001 2521\nm_lMd -1.017 -0.736 -0.500 1.001 2287\ns     -0.274 -0.087  0.082 1.000 2242\ns_lMd -0.432 -0.238 -0.056 1.001 2363\nt0    -0.265 -0.220 -0.195 1.001 2216\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -1.016 -0.880 -0.741 1.001 2491\nm_lMd -0.955 -0.700 -0.475 1.001 2315\ns     -0.338 -0.143  0.025 1.002 2278\ns_lMd -0.418 -0.218 -0.030 1.000 2516\nt0    -0.277 -0.225 -0.196 1.002 2295\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -1.249 -1.075 -0.873 1.001 2390\nm_lMd -1.393 -0.975 -0.641 1.001 2178\ns      0.048  0.196  0.335 1.002 2467\ns_lMd -0.552 -0.321 -0.110 1.001 2025\nt0    -0.089 -0.074 -0.066 1.002 2308\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -1.116 -0.948 -0.768 1.001 2330\nm_lMd -1.238 -0.858 -0.559 1.001 2229\ns     -0.283 -0.050  0.151 1.000 2518\ns_lMd -0.508 -0.269 -0.040 1.001 2186\nt0    -0.145 -0.095 -0.073 1.000 2468\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.317 -0.125 -0.005 1.000 2154\nm_lMd -0.340 -0.247 -0.180 1.002 2159\ns     -1.488 -1.321 -1.070 1.000 2300\ns_lMd -0.224 -0.048  0.129 1.000 2485\nt0    -2.761 -1.763 -1.129 1.000 2179\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.283 -0.102  0.012 1.002 2074\nm_lMd -0.318 -0.225 -0.159 1.002 2278\ns     -1.435 -1.265 -1.014 1.001 2173\ns_lMd -0.395 -0.215 -0.055 1.001 2350\nt0    -2.736 -1.790 -1.153 1.001 2102\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.424 -0.266 -0.087 1.000 2222\nm_lMd -0.536 -0.383 -0.264 1.000 2485\ns     -1.073 -0.815 -0.566 0.999 2226\ns_lMd -0.479 -0.302 -0.143 1.000 2285\nt0    -1.587 -1.023 -0.765 1.000 2222\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.408 -0.261 -0.074 1.000 2151\nm_lMd -0.549 -0.403 -0.290 1.003 2382\ns     -1.067 -0.813 -0.602 1.001 2080\ns_lMd -0.470 -0.297 -0.135 1.001 2099\nt0    -1.553 -1.002 -0.768 1.001 2192\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.392 -0.193 -0.096 1.002 2116\nm_lMd -0.292 -0.205 -0.144 1.000 1880\ns     -1.689 -1.529 -1.260 1.000 2115\ns_lMd -0.520 -0.319 -0.124 1.001 2362\nt0    -2.831 -1.979 -1.255 1.002 1805\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.369 -0.182 -0.080 1.005 2174\nm_lMd -0.296 -0.209 -0.148 1.000 2430\ns     -1.689 -1.526 -1.273 1.002 2255\ns_lMd -0.446 -0.240 -0.046 1.000 2470\nt0    -2.812 -1.942 -1.252 1.004 1962\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.582 -0.332 -0.190 1.002 2403\nm_lMd -0.300 -0.192 -0.128 1.002 2335\ns     -1.928 -1.715 -1.384 1.002 2155\ns_lMd -0.515 -0.269 -0.031 1.002 2378\nt0    -2.743 -1.772 -1.111 1.002 2424\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.648 -0.370 -0.213 1.000 2101\nm_lMd -0.294 -0.193 -0.123 1.000 2200\ns     -1.836 -1.609 -1.262 1.000 2293\ns_lMd -0.529 -0.291 -0.070 1.001 1926\nt0    -2.735 -1.706 -1.050 1.001 2131\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -0.125  0.555  0.987 1.004 1837\nv       1.666  2.431  3.128 1.005 2059\nv_lMd   0.727  0.984  1.348 1.004 2276\nA      -0.498  0.348  0.794 1.002 2065\nsv_lMd -0.553 -0.270 -0.016 1.003 2139\nt0     -2.553 -1.474 -0.911 1.005 2028\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -0.779  0.111  0.825 1.001 2185\nv       1.369  2.152  2.884 1.004 1992\nv_lMd   0.771  1.070  1.476 1.001 2431\nA      -0.855  0.310  0.775 1.000 2259\nsv_lMd -0.481 -0.069  0.327 1.000 2029\nt0     -1.807 -0.986 -0.627 1.001 2313\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -0.434  0.222  0.767 1.001 2173\nv       1.525  2.395  3.190 1.002 2119\nv_lMd   0.999  1.340  1.898 1.001 2170\nA       0.318  0.709  1.014 1.001 2305\nsv_lMd -0.824 -0.448 -0.145 1.000 2646\nt0     -1.864 -1.123 -0.795 1.002 1921\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -0.455  0.193  0.822 1.000 1912\nv       1.643  2.408  3.156 1.000 2062\nv_lMd   0.856  1.142  1.575 1.000 2272\nA      -0.068  0.485  0.816 1.000 2447\nsv_lMd -0.631 -0.268  0.018 1.001 1882\nt0     -1.754 -1.018 -0.725 1.000 1975\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -1.345 -0.083  0.706 1.002 1661\nv       1.160  2.142  3.125 1.002 1801\nv_lMd   0.597  0.896  1.311 1.002 2177\nA      -0.333  0.458  0.960 1.002 1876\nsv_lMd -0.482 -0.092  0.281 1.000 2204\nt0     -1.833 -0.969 -0.553 1.004 1631\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -1.687 -0.151  0.648 1.001 1848\nv       0.652  1.707  2.854 1.002 2293\nv_lMd   0.928  1.518  2.413 1.001 2061\nA      -0.704  0.344  0.900 1.002 1867\nsv_lMd -1.379 -0.677  0.212 1.001 1930\nt0     -1.764 -0.908 -0.461 1.002 1793\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -2.368 -0.876  0.194 1.002 2213\nv       0.289  1.393  2.600 1.002 1997\nv_lMd   0.736  1.182  2.062 1.002 2009\nA      -0.550  0.235  0.850 1.000 2015\nsv_lMd -0.899 -0.394  0.067 1.000 2160\nt0     -1.221 -0.639 -0.402 1.002 2047\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -1.721 -0.048  0.701 1.001 2097\nv       0.357  1.416  2.487 1.006 1896\nv_lMd   0.736  1.428  2.546 1.002 1974\nA      -1.300  0.078  0.767 1.001 1665\nsv_lMd -1.613 -0.813  0.254 1.000 2024\nt0     -1.953 -0.988 -0.443 1.001 2116\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -1.669 -0.337  0.538 1.001 2336\nv       0.999  1.964  2.773 1.003 2228\nv_lMd   0.975  1.401  2.161 1.002 2496\nA       0.190  0.511  0.810 1.003 2127\nsv_lMd -0.910 -0.498 -0.122 1.001 1999\nt0     -1.464 -0.790 -0.474 1.000 2386\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -1.700 -0.341  0.663 1.002 2043\nv       1.368  2.373  3.317 1.001 1959\nv_lMd   0.771  1.218  1.780 1.001 2259\nA       0.270  0.697  1.038 1.002 2093\nsv_lMd -1.051 -0.457  0.179 1.002 2144\nt0     -1.534 -0.734 -0.449 1.002 2191\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -0.200  0.427  0.913 1.002 2197\nv       2.022  2.670  3.327 1.001 2103\nv_lMd   0.640  0.891  1.208 1.000 2100\nA      -0.060  0.582  0.937 1.002 2268\nsv_lMd -0.491 -0.191  0.060 1.002 2245\nt0     -2.223 -1.277 -0.843 1.002 2088\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -0.106  0.515  0.956 1.005 2086\nv       1.589  2.491  3.232 1.004 2170\nv_lMd   0.897  1.219  1.787 1.002 2233\nA      -0.179  0.460  0.836 1.001 2323\nsv_lMd -1.027 -0.597 -0.237 1.002 1979\nt0     -2.364 -1.402 -0.925 1.006 1800\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -1.509 -0.152  0.764 1.001 2489\nv       1.194  2.104  2.907 1.004 2447\nv_lMd   0.861  1.206  1.703 1.002 2684\nA      -0.387  0.422  0.864 1.001 2154\nsv_lMd -0.543 -0.165  0.279 1.002 2153\nt0     -1.694 -0.800 -0.430 1.002 2227\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -1.387 -0.155  0.680 1.002 1950\nv       0.675  1.739  2.621 1.002 1999\nv_lMd   0.597  1.196  2.064 1.000 2164\nA      -1.195  0.311  0.869 1.003 2168\nsv_lMd -1.391 -0.698  0.083 1.000 1882\nt0     -1.918 -0.968 -0.531 1.001 2148\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -1.760 -0.230  0.735 1.000 2057\nv       1.144  2.152  3.014 1.000 1999\nv_lMd   0.630  0.925  1.329 1.003 1977\nA      -1.174  0.133  0.775 1.003 2018\nsv_lMd -0.393  0.009  0.508 1.000 2123\nt0     -1.500 -0.711 -0.375 1.000 1921\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -1.841 -0.251  0.782 1.001 2112\nv       0.753  1.832  2.775 1.000 2075\nv_lMd   0.590  1.296  2.026 1.006 2352\nA      -1.192  0.289  0.856 1.003 2060\nsv_lMd -1.425 -0.552  0.270 1.001 1923\nt0     -1.704 -0.746 -0.361 1.000 2267\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -0.274  0.344  0.861 1.001 2111\nv       1.746  2.541  3.438 1.001 2177\nv_lMd   0.747  1.014  1.352 1.001 2471\nA       0.057  0.586  0.988 1.001 2345\nsv_lMd -0.620 -0.328 -0.063 1.000 2407\nt0     -1.996 -1.220 -0.863 1.002 2118\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -0.128  0.416  0.889 1.000 2396\nv       1.481  2.253  3.044 1.001 2224\nv_lMd   0.844  1.157  1.585 1.002 2421\nA      -0.288  0.426  0.861 1.001 2347\nsv_lMd -0.675 -0.353 -0.060 1.001 2348\nt0     -2.380 -1.402 -0.982 1.001 2594\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -0.158  0.416  0.933 1.002 2012\nv       1.740  2.386  3.048 1.002 2183\nv_lMd   0.707  0.955  1.281 1.001 1929\nA      -0.791  0.325  0.816 1.000 1916\nsv_lMd -0.497 -0.222  0.012 1.001 2284\nt0     -2.221 -1.295 -0.891 1.001 1951\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -0.166  0.371  0.884 1.004 1902\nv       1.524  2.256  2.973 1.000 2263\nv_lMd   0.760  1.054  1.500 1.000 2442\nA      -0.508  0.414  0.854 1.001 2363\nsv_lMd -0.774 -0.440 -0.176 1.001 2270\nt0     -2.287 -1.303 -0.921 1.003 2372\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -0.749 -0.008  0.656 1.003 2175\nv       0.936  1.897  2.904 1.001 2204\nv_lMd   0.713  1.007  1.465 1.003 2195\nA      -0.628  0.278  0.837 1.001 2253\nsv_lMd -0.467 -0.162  0.140 1.001 2412\nt0     -1.741 -1.048 -0.755 1.003 2285\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -0.486  0.137  0.672 1.001 2025\nv       0.512  1.351  2.185 1.001 2215\nv_lMd   0.757  1.202  1.983 1.001 2146\nA      -1.858 -0.379  0.440 1.002 2112\nsv_lMd -0.734 -0.407 -0.119 1.001 2214\nt0     -1.878 -1.228 -0.884 1.001 2053\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -0.353  0.240  0.773 1.002 2133\nv       0.657  1.589  2.649 1.003 2533\nv_lMd   0.893  1.244  1.893 1.001 2240\nA      -1.516 -0.075  0.683 1.002 2111\nsv_lMd -0.633 -0.347 -0.091 1.000 2165\nt0     -2.118 -1.309 -0.945 1.001 2098\n\n alpha 1 \n         2.5%    50%  97.5%  Rhat  ESS\nB      -0.593  0.119  0.707 1.001 2045\nv       0.362  1.359  2.472 1.002 2176\nv_lMd   0.811  1.399  2.407 1.001 2255\nA      -1.528 -0.095  0.608 1.001 2330\nsv_lMd -0.954 -0.531 -0.143 1.001 2162\nt0     -1.928 -1.189 -0.833 1.004 2163\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.312 -0.113  0.060 1.001 2080\nm_lMd -0.448 -0.309 -0.215 1.002 2037\ns     -1.118 -0.894 -0.614 1.003 2024\ns_lMd -0.269 -0.106  0.052 1.001 1860\nt0    -2.312 -1.359 -0.896 1.001 2052\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.433 -0.250 -0.044 1.002 2235\nm_lMd -0.630 -0.440 -0.308 1.000 2410\ns     -1.026 -0.740 -0.467 1.003 2165\ns_lMd -0.542 -0.303 -0.079 1.000 2200\nt0    -1.614 -0.999 -0.725 1.002 2286\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.466 -0.338 -0.185 1.000 2218\nm_lMd -0.629 -0.465 -0.341 1.000 2746\ns     -0.828 -0.614 -0.430 0.999 2303\ns_lMd -0.275 -0.108  0.044 1.001 2345\nt0    -1.087 -0.813 -0.667 1.000 2295\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.399 -0.260 -0.084 1.000 2273\nm_lMd -0.695 -0.515 -0.373 1.001 2469\ns     -0.944 -0.699 -0.482 1.001 2244\ns_lMd -0.535 -0.326 -0.136 1.003 2278\nt0    -1.377 -0.945 -0.739 1.001 2386\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.602 -0.392 -0.127 1.001 2179\nm_lMd -0.843 -0.558 -0.365 1.001 2333\ns     -0.929 -0.543 -0.207 1.001 2174\ns_lMd -0.473 -0.289 -0.110 1.001 2168\nt0    -1.279 -0.731 -0.514 1.001 2092\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.527 -0.277 -0.012 1.001 2483\nm_lMd -0.650 -0.399 -0.242 1.001 2272\ns     -1.048 -0.693 -0.281 1.000 2339\ns_lMd -0.693 -0.285  0.074 1.000 2332\nt0    -1.784 -0.926 -0.579 1.001 2244\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.595 -0.271  0.014 1.004 2173\nm_lMd -0.794 -0.455 -0.288 1.002 2259\ns     -1.191 -0.784 -0.279 1.005 2176\ns_lMd -0.251 -0.045  0.144 1.000 2163\nt0    -2.022 -0.949 -0.514 1.008 2164\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.804 -0.520 -0.196 1.000 2139\nm_lMd -1.328 -0.732 -0.383 1.000 2189\ns     -0.743 -0.155  0.397 1.000 2014\ns_lMd -1.418 -0.850 -0.234 1.002 2172\nt0    -1.040 -0.505 -0.320 1.001 2162\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.469 -0.265 -0.040 1.003 2224\nm_lMd -0.585 -0.402 -0.271 1.001 2137\ns     -1.010 -0.731 -0.451 1.002 1971\ns_lMd -0.389 -0.220 -0.057 1.001 2100\nt0    -1.641 -0.974 -0.680 1.004 2192\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.557 -0.386 -0.178 1.001 2222\nm_lMd -0.761 -0.555 -0.390 1.001 2303\ns     -0.795 -0.542 -0.307 1.002 2331\ns_lMd -0.495 -0.222  0.019 1.001 2130\nt0    -1.094 -0.745 -0.574 1.003 2154\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.389 -0.248 -0.072 1.001 2270\nm_lMd -0.652 -0.484 -0.348 1.001 1841\ns     -0.871 -0.635 -0.428 1.002 2140\ns_lMd -0.409 -0.241 -0.079 1.000 2650\nt0    -1.359 -0.932 -0.730 1.003 2325\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.422 -0.276 -0.103 1.001 2250\nm_lMd -0.577 -0.431 -0.312 1.001 2165\ns     -0.970 -0.730 -0.520 1.000 2258\ns_lMd -0.343 -0.160  0.018 1.000 2451\nt0    -1.378 -0.942 -0.730 1.001 2230\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.455 -0.165  0.059 1.003 2169\nm_lMd -0.685 -0.460 -0.324 1.002 2179\ns     -1.055 -0.788 -0.454 1.003 2169\ns_lMd -0.383 -0.180  0.023 1.000 2268\nt0    -2.100 -1.107 -0.627 1.003 2135\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.766 -0.460 -0.151 1.002 2275\nm_lMd -0.754 -0.489 -0.279 1.003 2252\ns     -0.950 -0.617 -0.289 1.001 2131\ns_lMd -0.529 -0.103  0.231 1.003 2258\nt0    -1.281 -0.709 -0.451 1.002 1894\n\n alpha 1 \n        2.5%    50%  97.5% Rhat  ESS\nm     -0.655 -0.233  0.052    1 2173\nm_lMd -0.572 -0.330 -0.207    1 2180\ns     -1.056 -0.755 -0.294    1 2183\ns_lMd -0.572 -0.345 -0.135    1 2181\nt0    -2.069 -0.975 -0.449    1 2241\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.809 -0.380 -0.033 1.004 2230\nm_lMd -1.003 -0.581 -0.275 1.002 2152\ns     -0.969 -0.602 -0.171 1.004 2227\ns_lMd -0.598 -0.067  0.346 1.001 2318\nt0    -1.556 -0.723 -0.357 1.004 2266\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.322 -0.189 -0.021 1.001 2213\nm_lMd -0.664 -0.483 -0.345 1.001 2070\ns     -1.034 -0.775 -0.548 1.000 2270\ns_lMd -0.508 -0.334 -0.167 1.000 2335\nt0    -1.569 -1.019 -0.789 1.001 2225\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.389 -0.272 -0.131 1.002 2377\nm_lMd -0.710 -0.512 -0.354 1.001 2387\ns     -0.768 -0.520 -0.303 1.001 2340\ns_lMd -0.406 -0.221 -0.044 1.003 2241\nt0    -1.129 -0.827 -0.682 1.002 2565\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.422 -0.300 -0.129 1.002 2359\nm_lMd -0.550 -0.403 -0.285 1.003 2221\ns     -0.962 -0.730 -0.539 1.003 2411\ns_lMd -0.381 -0.220 -0.070 1.001 2173\nt0    -1.312 -0.923 -0.749 1.002 2291\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.344 -0.220 -0.065 1.002 2241\nm_lMd -0.580 -0.436 -0.318 1.000 2446\ns     -1.011 -0.786 -0.595 1.002 2175\ns_lMd -0.405 -0.240 -0.073 1.001 2349\nt0    -1.464 -1.013 -0.813 1.003 2418\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.385 -0.270 -0.102 1.001 2211\nm_lMd -0.726 -0.518 -0.353 1.001 2542\ns     -0.988 -0.680 -0.424 1.001 2177\ns_lMd -0.463 -0.297 -0.129 1.000 2987\nt0    -1.355 -0.893 -0.716 1.002 2306\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.460 -0.329 -0.138 1.003 2002\nm_lMd -0.536 -0.361 -0.230 1.001 2188\ns     -1.019 -0.681 -0.425 1.002 2263\ns_lMd -0.326 -0.115  0.085 1.000 2453\nt0    -1.391 -0.877 -0.700 1.004 2251\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.390 -0.245 -0.042 1.003 1947\nm_lMd -0.647 -0.424 -0.271 1.001 1939\ns     -1.229 -0.831 -0.483 1.002 1855\ns_lMd -0.474 -0.302 -0.134 1.002 2437\nt0    -1.902 -1.041 -0.754 1.004 1936\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nm     -0.469 -0.335 -0.157 1.002 2211\nm_lMd -0.851 -0.532 -0.315 1.000 2368\ns     -1.024 -0.632 -0.315 1.000 2382\ns_lMd -0.545 -0.287 -0.040 1.001 2406\nt0    -1.271 -0.825 -0.663 1.002 2484\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      0.466  1.217  1.347 1.004 1754\nv      0.465  1.002  1.174 1.003 1781\nv_lMd  0.326  0.476  0.923 1.004 1982\ns_lMd -0.463 -0.279 -0.074 1.001 2027\nt0    -2.861 -2.074 -0.749 1.005 1745\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.471  1.509  1.543 1.005 2632\nv      1.342  1.374  1.410 1.010 2861\nv_lMd  0.151  0.196  0.249 1.003 2321\ns_lMd  0.019  0.094  0.150 1.007 2314\nt0    -2.985 -2.737 -2.255 1.000 1789\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.205  1.307  1.398 1.001 2421\nv      0.990  1.115  1.220 1.001 2456\nv_lMd  0.298  0.394  0.513 1.000 2460\ns_lMd -0.434 -0.280 -0.140 1.000 2388\nt0    -2.925 -2.376 -1.867 1.000 2136\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.125  1.240  1.349 1.000 2238\nv      0.941  1.071  1.179 1.000 2075\nv_lMd  0.289  0.390  0.525 1.001 2027\ns_lMd -0.566 -0.410 -0.254 1.000 2266\nt0    -2.832 -2.140 -1.670 1.000 2213\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      0.685  1.325  1.421 1.001 2340\nv      0.687  1.125  1.242 1.001 2327\nv_lMd  0.219  0.314  0.524 1.001 2346\ns_lMd -0.417 -0.259 -0.059 1.001 2192\nt0    -2.958 -2.477 -0.894 1.001 2274\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.286  1.508  1.559 1.004 2973\nv      1.140  1.393  1.445 1.003 3056\nv_lMd  0.153  0.209  0.330 1.001 2680\ns_lMd -0.315  0.105  0.193 1.001 2685\nt0    -2.974 -2.634 -2.016 1.001 2030\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB     -0.225  0.178  0.785 1.000 2280\nv     -0.441  0.310  0.868 1.002 2334\nv_lMd  0.465  0.948  1.958 1.001 2281\ns_lMd -0.319 -0.123  0.057 1.001 2258\nt0    -0.960 -0.503 -0.344 1.001 2264\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.451  1.506  1.594 1.000 2953\nv      1.371  1.419  1.516 1.003 2813\nv_lMd  0.184  0.243  0.304 1.000 2839\ns_lMd  0.055  0.147  0.318 1.002 2980\nt0    -2.951 -2.449 -1.871 1.002 3055\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      0.929  1.249  1.351 1.002 2509\nv      0.887  1.078  1.189 1.002 2415\nv_lMd  0.289  0.389  0.543 1.001 2452\ns_lMd -0.529 -0.374 -0.226 1.001 2312\nt0    -2.858 -2.193 -1.227 1.002 2294\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.063  1.233  1.415 1.005 1953\nv      0.968  1.117  1.288 1.002 2171\nv_lMd  0.203  0.318  0.451 1.002 1945\ns_lMd -0.644 -0.414 -0.109 1.003 1916\nt0    -2.830 -2.015 -1.471 1.008 2064\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.187  1.289  1.374 1.002 2398\nv      0.983  1.103  1.208 1.001 2270\nv_lMd  0.278  0.364  0.481 1.001 2487\ns_lMd -0.468 -0.324 -0.186 1.002 2506\nt0    -2.922 -2.383 -1.846 1.000 2131\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      0.876  1.177  1.308 1.010 2595\nv      0.820  1.031  1.155 1.031 2414\nv_lMd  0.327  0.447  0.645 1.001 2332\ns_lMd -0.650 -0.484 -0.316 1.001 1975\nt0    -2.565 -1.860 -1.159 1.000 2537\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      0.325  1.179  1.317 1.001 1724\nv      0.397  0.948  1.129 1.000 1819\nv_lMd  0.319  0.477  0.978 1.000 1683\ns_lMd -0.590 -0.383 -0.131 1.002 2190\nt0    -2.822 -1.977 -0.557 1.001 1570\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      0.054  1.028  1.249 1.003 1456\nv      0.158  0.873  1.091 1.003 1436\nv_lMd  0.190  0.355  1.029 1.002 1674\ns_lMd -0.934 -0.634 -0.177 1.001 2378\nt0    -2.677 -1.714 -0.451 1.002 1366\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.147  1.305  1.551 1.004 1605\nv      0.988  1.157  1.426 1.003 1503\nv_lMd  0.167  0.331  0.468 1.002 1604\ns_lMd -0.506 -0.286  0.107 1.002 1480\nt0    -2.921 -2.197 -1.510 1.001 2331\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      0.739  1.042  1.474 1.001 1858\nv      0.627  0.899  1.282 1.004 1852\nv_lMd  0.159  0.324  0.564 1.004 2144\ns_lMd -1.365 -0.875 -0.013 1.001 1662\nt0    -2.815 -1.727 -1.034 1.001 1855\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      0.766  1.221  1.318 1.000 2649\nv      0.592  0.969  1.106 1.003 2549\nv_lMd  0.398  0.534  0.864 1.001 2737\ns_lMd -0.588 -0.418 -0.252 1.000 2329\nt0    -2.916 -2.338 -1.125 1.000 2405\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      0.461  1.073  1.295 1.000 1793\nv      0.357  0.888  1.141 1.001 1864\nv_lMd  0.321  0.537  1.068 1.001 1968\ns_lMd -0.577 -0.411 -0.254 1.001 2185\nt0    -2.588 -1.648 -0.806 1.000 1817\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.054  1.223  1.301 1.003 2500\nv      0.816  0.988  1.101 1.002 2413\nv_lMd  0.337  0.451  0.609 1.001 2383\ns_lMd -0.589 -0.445 -0.306 1.000 2150\nt0    -2.962 -2.519 -1.793 1.001 2166\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.164  1.268  1.368 1.001 2328\nv      1.010  1.121  1.222 1.000 2297\nv_lMd  0.258  0.341  0.452 1.000 2286\ns_lMd -0.507 -0.364 -0.227 1.001 2335\nt0    -2.898 -2.237 -1.753 1.002 2066\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.331  1.503  1.551 1.003 2435\nv      1.205  1.403  1.454 1.003 2592\nv_lMd  0.127  0.171  0.284 1.003 2405\ns_lMd -0.210  0.047  0.111 1.003 2471\nt0    -2.986 -2.715 -2.149 1.001 2037\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.185  1.343  1.539 1.000 1335\nv      1.066  1.229  1.453 1.000 1324\nv_lMd  0.062  0.202  0.344 1.001 1371\ns_lMd -0.477 -0.259  0.052 1.000 1318\nt0    -2.973 -2.408 -1.709 1.000 1448\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.123  1.246  1.374 1.003 2209\nv      0.949  1.099  1.237 1.001 2281\nv_lMd  0.285  0.385  0.533 1.000 2241\ns_lMd -0.568 -0.412 -0.272 1.001 2186\nt0    -2.851 -2.142 -1.633 1.001 2284\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.454  1.489  1.538 1.002 2341\nv      1.376  1.408  1.462 1.001 2334\nv_lMd  0.071  0.115  0.160 1.003 2798\ns_lMd -0.037  0.044  0.101 1.001 2870\nt0    -2.977 -2.691 -2.193 1.002 2458\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.066  1.544  1.666 1.001 1941\nv      1.113  1.490  1.569 1.001 1989\nv_lMd  0.166  0.233  0.502 1.001 1879\ns_lMd -0.473 -0.203 -0.031 1.001 2337\nt0    -2.880 -1.973 -1.073 1.001 1798\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.117  1.555  1.639 1.000 1884\nv      1.162  1.493  1.553 1.000 1975\nv_lMd  0.122  0.173  0.383 1.000 2071\ns_lMd -0.450 -0.159 -0.029 1.001 2491\nt0    -2.921 -2.275 -1.201 1.000 1646\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.256  1.501  1.561 1.000 1892\nv      1.135  1.387  1.440 1.000 2012\nv_lMd  0.135  0.189  0.359 1.001 2150\ns_lMd -0.327  0.011  0.099 1.000 2021\nt0    -2.964 -2.547 -1.770 1.001 1890\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      0.775  1.232  1.351 1.001 2862\nv      0.764  1.059  1.182 1.002 2672\nv_lMd  0.325  0.437  0.683 1.000 2877\ns_lMd -0.548 -0.388 -0.229 1.002 2244\nt0    -2.754 -2.031 -1.016 1.001 2678\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.277  1.567  1.769 1.006 1569\nv      1.531  1.686  1.772 1.005 1772\nv_lMd  0.152  0.201  0.274 1.004 1631\ns_lMd -0.416 -0.226 -0.029 1.001 1292\nt0    -2.217 -1.397 -0.920 1.004 1471\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.351  1.546  1.745 1.001 2122\nv      1.607  1.678  1.769 1.003 2182\nv_lMd  0.122  0.168  0.220 1.004 2376\ns_lMd -0.381 -0.196 -0.011 1.000 2130\nt0    -2.069 -1.361 -0.987 1.001 2377\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.007  1.275  1.642 1.000 2188\nv      1.778  1.907  2.046 1.000 2654\nv_lMd  0.132  0.193  0.275 1.000 2629\ns_lMd -0.323 -0.095  0.107 1.001 2080\nt0    -0.910 -0.589 -0.441 1.000 2224\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.044  1.317  1.678 1.003 2243\nv      1.807  1.918  2.066 1.001 2293\nv_lMd  0.111  0.169  0.241 1.002 2467\ns_lMd -0.387 -0.149  0.068 1.000 2429\nt0    -0.951 -0.634 -0.465 1.002 2233\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      0.073  0.292  0.581 1.001 2228\nv      0.396  0.671  0.912 1.000 2237\nv_lMd  0.482  0.689  1.013 1.001 2295\ns_lMd -0.090  0.074  0.223 1.001 2327\nt0    -0.604 -0.461 -0.385 1.001 2154\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      0.012  0.228  0.501 1.000 2381\nv      0.065  0.483  0.775 1.001 2281\nv_lMd  0.613  0.962  1.603 1.000 2307\ns_lMd -0.329 -0.174 -0.025 1.001 2227\nt0    -0.565 -0.435 -0.366 1.001 2481\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      0.548  1.229  1.383 1.002 2182\nv      0.752  1.181  1.299 1.002 2182\nv_lMd  0.210  0.307  0.574 1.002 2100\ns_lMd -0.456 -0.309 -0.141 1.001 2379\nt0    -2.196 -1.582 -0.692 1.001 2284\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      0.368  0.626  0.955 1.002 1886\nv      0.359  0.668  0.917 1.002 2061\nv_lMd  0.464  0.698  1.070 1.002 2280\ns_lMd -0.356 -0.194 -0.041 1.003 2288\nt0    -1.228 -0.822 -0.640 1.004 1872\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB     -0.319 -0.136  0.085 1.001 2524\nv     -0.277  0.272  0.625 1.002 2180\nv_lMd  0.773  1.249  2.150 1.002 2236\ns_lMd -0.217 -0.048  0.117 1.001 2179\nt0    -0.225 -0.173 -0.143 1.001 2524\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB     -0.356 -0.169  0.073 1.002 2257\nv     -0.208  0.276  0.605 1.004 2314\nv_lMd  0.612  1.029  1.839 1.005 2318\ns_lMd -0.216 -0.070  0.085 0.999 2335\nt0    -0.241 -0.185 -0.153 1.001 2277\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB     -0.459 -0.242  0.025 1.002 2451\nv     -0.399  0.206  0.636 1.001 2615\nv_lMd  0.932  1.535  2.595 1.001 2580\ns_lMd -0.108  0.100  0.302 1.001 2453\nt0    -0.104 -0.056 -0.028 1.002 2343\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB     -0.592 -0.374 -0.114 1.001 2736\nv     -0.404  0.228  0.668 1.000 2408\nv_lMd  0.771  1.357  2.463 1.001 2510\ns_lMd -0.198  0.004  0.193 1.000 1999\nt0    -0.081 -0.040 -0.016 1.001 2197\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.456  1.551  1.619 1.000 2146\nv      1.447  1.498  1.555 1.001 2106\nv_lMd  0.150  0.191  0.234 1.001 1912\ns_lMd -0.161 -0.058  0.061 1.000 2201\nt0    -2.956 -2.467 -1.862 1.001 2054\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.092  1.288  1.591 1.001 1548\nv      1.064  1.234  1.525 1.002 1589\nv_lMd  0.191  0.370  0.519 1.001 1849\ns_lMd -0.561 -0.388 -0.112 1.001 1934\nt0    -2.895 -1.867 -1.367 1.001 1652\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.153  1.272  1.505 1.003 1900\nv      1.083  1.194  1.409 1.003 2034\nv_lMd  0.156  0.283  0.390 1.002 2016\ns_lMd -0.469 -0.321 -0.050 1.000 2145\nt0    -2.747 -1.866 -1.480 1.002 2127\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.111  1.226  1.353 1.001 2129\nv      0.961  1.093  1.214 1.000 2225\nv_lMd  0.288  0.389  0.523 1.000 2058\ns_lMd -0.568 -0.419 -0.271 1.001 2112\nt0    -2.775 -2.035 -1.607 1.001 2239\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.518  1.637  1.709 1.003 2543\nv      1.592  1.673  1.734 1.003 2470\nv_lMd  0.132  0.178  0.234 1.000 2386\ns_lMd -0.464 -0.326 -0.194 1.002 2221\nt0    -2.960 -2.546 -1.951 1.000 2069\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.412  1.567  1.729 1.001 2030\nv      1.622  1.697  1.789 1.001 1878\nv_lMd  0.093  0.137  0.184 1.001 2395\ns_lMd -0.379 -0.221 -0.056 1.000 2126\nt0    -2.803 -1.911 -1.415 1.001 2225\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.469  1.623  1.751 1.000 2123\nv      1.694  1.774  1.877 1.003 2366\nv_lMd  0.082  0.134  0.186 1.003 2419\ns_lMd -0.494 -0.329 -0.147 0.999 2263\nt0    -2.889 -2.218 -1.626 0.999 2177\n\n alpha 1 \n        2.5%    50%  97.5%  Rhat  ESS\nB      1.355  1.577  1.801 1.000 2231\nv      1.699  1.800  1.942 1.000 2172\nv_lMd  0.114  0.173  0.235 1.001 2152\ns_lMd -0.396 -0.195  0.015 1.002 2197\nt0    -2.748 -1.779 -1.270 1.000 2266\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\npars_EMCs_both$Missing <- factor(pars_EMCs_both$Missing,\n                                     c(2,10,30,50),\n                                     c(\"2%\",\"10%\", \"30%\", \"50%\"))\n```\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\nWasserstein_both <- data.frame()\n\nfor (i in 1:nrow(EMCs_both)) {\n  MCMC_sample <- get_pars(get(EMCs_both$EMC[i], envir = globalenv()),\n                          return_mcmc = F, merge_chains = T)\n  true_pars <- get(paste0(\"pars\", as.character(EMCs_both$Model[i])))\n  W <- rbind(model = sqrt(rowMeans(sqrt(colSums((MCMC_sample[,1:10,] - true_pars)^2)^2))),\n           sqrt(apply((MCMC_sample[,1:10,] - true_pars)^2, c(1,2), mean)))\n  W_column <- data.frame(W = c(W), Variable = rownames(W)[row(W)],\n                         id = colnames(W)[col(W)])\n  W_addition <- cbind(EMCs_both[i,], W_column,\n                                     row.names = NULL)\n  Wasserstein_both <- rbind(Wasserstein_both, W_addition)\n}\n\nWasserstein_both$Missing <- factor(Wasserstein_both$Missing,\n                                     c(2,10,30,50),\n                                     c(\"2%\",\"10%\", \"30%\", \"50%\"))\n```\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\n# Calculate RMSE per group\nEMCs_both_RMSEs <- pars_EMCs_both %>%\n  group_by(id, Missing, Response, \n           Tail, Censoring, Model) %>%\n  summarise(\n    RMSE = sqrt(mean((Median - True)^2)),  # Calculate RMSE\n    .groups = 'drop'  # Ungroup after summarizing\n  )\n```\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\n# Calculate MAE per group\nEMCs_both_MAEs <- pars_EMCs_both %>%\n  group_by(id, Missing, Response, \n           Tail, Censoring, Model) %>%\n  summarise(\n    MAE = mean(abs(Median - True)),  # Calculate MAE\n    .groups = 'drop'  # Ungroup after summarizing\n  )\n```\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\n# Calculate R per group\nEMCs_both_Rs <- pars_EMCs_both %>%\n  group_by(id, Missing, Response, \n           Tail, Censoring, Model) %>%\n  summarise(\n    R = cor(Median, True),  # Calculate correlation\n    .groups = 'drop'  # Ungroup after summarizing\n  )\n```\n:::\n\n\n\n\nMany paradigms in experimental psychology involve speeded decision-making. There are two main outcome variables to these tasks: what choice someone made (or whether this matches the corresponding stimulus), and how fast someone made their choice. Researchers are often interested in the conditions that affect these decisions and response times (RTs).\n\nA problem arises when we want to make comparisons about performance on a task. Someone might be quicker to respond---indicating better performance, but at the same time they might be less accurate---indicating worse performance. This is commonly referred to as the speed accuracy trade-off, which complicates inferences on task performance.\n\nA wide range of evidence accumulation models (EAMs) aim to model the cognitive processes behind decision making as noisy accumulation of evidence until a decision threshold is reached. This means that how fast a participant is able to accumulate evidence towards a choice (the evidence accumulation rate or drift rate) is modeled separately from a participants’ tendency to value speed over accuracy or vice versa (the threshold or speed-accuracy tradeoff). In addition to these decisional variables, EAMs often estimate the time it takes for non-decisional processes like stimulus encoding or the motor response to occur (non-decision time), and they account for the variability within and between trials, as well as response bias.\n\nThis paper will focus on three prominent EAMs that are supported by the EMC2 package [@EMC2]: the Linear Ballistic Accumulator [LBA\\; @LBA], the Racing Diffusion Model [RDM\\; @RDM], and the Log-Normal Race [LNR\\; @LNR]. These models are race models, which model separate accumulators for each choice option, with the first accumulator to reach a threshold resulting in the choice. The time that it takes for this accumulator to reach the threshold is the decision time.\n\nThe LBA [@LBA] models RTs and responses as a race between deterministic linear accumulators, with the slope for an accumulator drawn from a normal distribution, $\\mathcal{N}(v, s_{v}^2)$, where $v$ is the mean evidence accumulation rate and $s_v^2$ the between-trial variance of the evidence accumulation rate. The intercept also has between-trial variation and is drawn from a uniform distribution $\\mathcal{U}(0,A)$. The distance from $A$ to the threshold $b$ is called $B$, and can be understood as caution, or the tradeoff between speed and accuracy. The time it takes for non-decsisional processes like motor response and stimulus encoding are estimated as a sigle non-decision time parameter, $t_0$. The line that intersects the threshold at the lowest time determines the decision made, and the timepoint of the intersection added to $t_0$ is the RT [@LBA\\; see @fig-racemodels].\n\nThe RDM [@RDM] has a similar set of parameters, but instead of having between-trial variation in evidence accumulation rate, the RDM has continuous normally distributed variation within each trial. In the simulation studies in this paper, starting point variability $A$ is taken out as it is often not a necessary parameter for the RDM to account for common decisional phenomena [@RDM\\; see @fig-racemodels].\n\nLastly, the LNR [@LNR] is a race between random samples from log-normal distributions, with the lowest sample determining the choice and decision time, which is then added to non-decision time $t_0$ to constitute the RT. This means that the LNR has three main parameters: the scale $m$ of the lognormal, the shape $s$ of the lognormal, and the non-decision time $t_0$. The LNR can not distinguish between the threshold and the accumulation rate, so it does not rest on latent assumptions about the accumulation process [@LNR\\; see @fig-racemodels].\n\n\n\n\n::: {#cell-fig-racemodels .cell apa-note='The dynamics of the race models in this paper are illustrated here. The Linear Ballistic Accumulator (LBA) accumulates evidence in a linear deterministic fashion, with normally distributed variability $s_v^2$ in slope $v$ and uniformly distributed variability $A$ in starting point between trials. The Racing Diffusion Model (RDM) has continuous normally distributed variance $s_v^2$ within the trial instead (diffusion), and can include starting point variability between trials like the LBA, although this is not the case for simulations in this paper. The Log-Normal Race (LNR) samples accumulation times (vertical lines) from log-normal distributions, with the shortest accumulation time (solid vertical lines) determining the choice and the decision time. For all models, the duration of non-decision processes like stimulus encoding and responding are captured by $t_0$.'}\n\n```{.r .cell-code .hidden}\n# LBA ----\n\nLBA_accumulator_plot <- ggplot(data.frame(RT = 0:2)) +\n  geom_segment(aes(x = 0.2, y = 1, xend = (4 - 1) / 5 , yend = 4,\n                   color = \"True\"),\n               linewidth = 0.8,\n               arrow = arrow(length = unit(3, \"mm\"), type = \"open\")) +\n  geom_segment(aes(x = 0.2, y = 1.3, xend = (4 - 1.3) / 2.5 , yend = 4,\n                   color = \"False\"),\n               linewidth = 0.8,\n               arrow = arrow(length = unit(3, \"mm\"), type = \"open\")) +\n  theme_classic() +\n  theme(plot.margin = margin(t = 0),\n        axis.text = element_blank(),\n        axis.title.y = element_blank(),\n        axis.ticks = element_blank(),\n        axis.line.y = element_blank(),\n        axis.line.x = element_line(arrow = arrow(length = unit(4, \"mm\"),\n                                                 type = \"open\")),\n        legend.position = \"none\") +\n  xlab(\"Response Time\") +\n  geom_hline(yintercept = 4, linewidth = 2) +\n  geom_vline(xintercept = 0.001,\n             linetype = \"dashed\") +\n  geom_vline(xintercept = 0.2,\n             linetype = \"dashed\") +\n  geom_segment(x = 0.01, xend = 0.19, y = 3.3, yend = 3.3, linewidth = 0.6) +\n  geom_segment(x = 0.01, xend = 0.01, y = 3.2, yend = 3.4) +\n  geom_segment(x = 0.19, xend = 0.19, y = 3.2, yend = 3.4) +\n  annotate(\"text\", x = 0.1, y  = 2.8, label = \"t[0]\", parse = TRUE) +\n  annotate(\"rect\", xmin = 0.1, xmax = 0.2, ymin = 0, ymax = 2, alpha = 0.2) +\n  annotate(\"text\", x = 0.15, y = 1, label = \"A\") +\n  geom_segment(x = 1.8, xend = 1.8, y = 0, yend = 4, \n               arrow = arrow(length = unit(3, \"mm\"), type = \"open\")) +\n  annotate(\"text\", x = 1.87, y = 2, label = \"b\") +\n  scale_x_continuous(limits=c(0, 2), expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_color_manual(values = ft_colors)\n\nLBA_density_plot <- ggplot(data.frame(RT = 0:2)) +\n  stat_function(fun = dLBA,\n                aes(color = \"True\"),\n                args = list(response = 2, A=2, b= 4, t0 = .2, mean_v=c(3,4), sd_v=c(1,1.75)),\n                linewidth = 0.8) +\n  stat_function(fun = dLBA,\n                aes(color = \"False\"),\n                args = list(response = 1, A=2, b= 4, t0 = .2, mean_v=c(3,4), sd_v=c(1,1.75)),\n                linewidth = 0.8) +\n  theme_classic() +\n  theme(axis.ticks = element_blank(),\n        axis.text = element_blank(),\n        axis.title.x = element_blank(),\n        plot.margin = margin(b = 0),\n        axis.ticks.length = unit(0, \"pt\"),\n        axis.line.y = element_blank(),\n        axis.title.y = element_blank(),\n        legend.position = \"none\") +\n  scale_x_continuous(limits=c(0, 2), expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  ggtitle(\"LBA\") +\n  scale_color_manual(values = ft_colors)\n\nLBA_plot <- LBA_density_plot / LBA_accumulator_plot\n\n# RDM ----\nparsRDM <- c(log(3),1,.4,log(.75),log(.2))\nnames(parsRDM) <- c(\"B\", \"v\", \"v_lMd\", \"s_lMd\", \"t0\")\n\nset.seed(2)\ndt = 0.001\naccumulator_true <- 0\ndrift_true <- 1.4\ns_true <- 0.75\nt0 <- 0.2\n\nwhile (accumulator_true[length(accumulator_true)] < 3) {\n  accumulator_true <- append(accumulator_true,\n                             accumulator_true[length(accumulator_true)] + \n                               dt * drift_true + rnorm(1, 0, sqrt(s_true^2 * dt)))\n}\n\naccumulator_false <- 0\ndrift_false <- 1\ns_false <- 1\n\nwhile (accumulator_false[length(accumulator_false)] < 3) {\n  accumulator_false <- append(accumulator_false,\n                             accumulator_false[length(accumulator_false)] + \n                               dt * drift_false + rnorm(1, 0, sqrt(s_false^2 * dt)))\n}\n\nRT_true <- t0 + seq(0, (length(accumulator_true) - 1) * dt, by = dt)\nRT_false <- t0 + seq(0, (length(accumulator_false) - 1) * dt, by = dt)\nRDM_dat <- data.frame(X = c(accumulator_true, accumulator_false), RT = c(RT_true, RT_false),\n           Accumulator = c(rep(\"True\", length(accumulator_true)),\n                           rep(\"False\", length(accumulator_false))))\n\nRDM_accumulator_plot <- ggplot(RDM_dat, aes(RT, X, color = Accumulator)) + \n  geom_line() +\n  theme_classic() +\n  theme(plot.margin = margin(t = 0, l = 8),\n        axis.text = element_blank(),\n        axis.title.y = element_blank(),\n        axis.ticks = element_blank(),\n        axis.line.y = element_blank(),\n        axis.line.x = element_blank(),\n        legend.position = \"none\") +\n  xlab(\"Response Time\") +\n  scale_x_continuous(limits=c(0, 4), expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  geom_segment(x = 0, xend = 4, y = 0, yend = 0, color = \"black\",\n               arrow = arrow(length = unit(4, \"mm\"), type = \"open\")) +\n  geom_hline(yintercept = 3, linewidth = 2) +\n  geom_vline(xintercept = 0.001,\n             linetype = \"dashed\") +\n  geom_vline(xintercept = 0.2,\n             linetype = \"dashed\") +\n  geom_segment(x = 0.01, xend = 0.19, y = 2.4, yend = 2.4, linewidth = 0.6, \n               color = \"black\") +\n  geom_segment(x = 0.01, xend = 0.01, y = 2.3, yend = 2.5,\n               color = \"black\") +\n  geom_segment(x = 0.19, xend = 0.19, y = 2.3, yend = 2.5,\n               color = \"black\") +\n  annotate(\"text\", x = 0.1, y  = 1.9, label = \"t[0]\", parse = TRUE) +\n  geom_segment(x = 3, xend = 3, y = 0, yend = 3, \n               arrow = arrow(length = unit(3, \"mm\"), type = \"open\"),\n               color = \"black\") +\n  annotate(\"text\", x = 3.13, y = 1.5, label = \"b\") +\n  geom_hline(yintercept = 0) +\n  scale_color_manual(values = ft_colors)\n\ndRDM <- function(t, v = 1.4,  B = 3, t0 = 0.2) {\n  t <- t - t0\n  out <- numeric(length(t))\n  out[t <= 0] <- 0\n  out[t > 0] <- B/sqrt(2*pi*t[t>0]^3) * \n    exp(-(1/2) * (v * t[t>0] - B)^2 / t[t>0])\n  out\n}\n\npRDM <- function(t, v = 1,  B = 3, t0 = 0.2) {\n  t <- t - t0\n  out <- numeric(length(t))\n  out[t <= 0] <- 0\n  out[t > 0] <- pnorm((v*t[t>0] - B)/sqrt(t[t>0])) + \n                exp(2*v*B)*pnorm((-v*t[t>0] - B) / sqrt(t[t>0]))\n  out\n}\n\ngRDM <- function(t, v1 = 1.4, v2 = 1,  B = 3, t0 = 0.2) {\n  dRDM(t, v1, B, t0) * (1 - pRDM(t, v2, B, t0))\n}\n\nRDM_density_plot <- ggplot(data.frame(RT = 0:6)) +\n  stat_function(fun = gRDM,\n                args = list(v1 = drift_true, v2 = drift_false,  B = 3, t0 = t0),\n                aes(color = \"True\"),\n                linewidth = .8) +\n  stat_function(fun = gRDM,\n                args = list(v1 = drift_false, v2 = drift_true,  B = 3, t0 = t0),\n                aes(color = \"False\"),\n                linewidth = .8) + \n  theme_classic() +\n  theme(axis.ticks = element_blank(),\n        axis.text = element_blank(),\n        axis.title.x = element_blank(),\n        plot.margin = margin(b = 0),\n        axis.ticks.length = unit(0, \"pt\"),\n        axis.line.y = element_blank(),\n        axis.title.y = element_blank(),\n        legend.position = \"none\") +\n  scale_x_continuous(limits=c(0, 5), expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  ggtitle(\"RDM\") +\n  scale_color_manual(values = ft_colors)\n\n\nRDM_plot <- RDM_density_plot / RDM_accumulator_plot\n\n# LNR ----\n\nparsLNR <- c(log(.75),log(.65),log(.5),log(.8),log(.4))\nnames(parsLNR) <- c(\"m\", \"m_lMd\", \"s\", \"s_lMd\", \"t0\")\n\nLNR_true <- function(x){\n  x <- x - 0.4\n  dlnorm(x, log(0.75) + log(0.65), 0.5 + 0.8) * \n    (1 - plnorm(x, log(0.75), 0.5))\n}\n\nLNR_false <- function(x){\n  x <- x - 0.4\n  dlnorm(x, log(0.75), 0.5) * (1 - plnorm(x, log(0.75) + log(0.65), 0.5 + 0.8))\n}\n\nset.seed(1312)\nsample_true <- rlnorm(50,  log(0.75) + log(0.65), 0.5 + 0.8)\nsample_false <- rlnorm(50,  log(0.75), 0.5)\nLNR_sample <- data.frame(RT = c(sample_true, sample_false) + 0.4, \n                         Choice = rep(c(\"Correct\", \"Incorrect\"), each = 50),\n                         Won = c(sample_true > sample_false, sample_true < sample_false))\n\nLNR_plot <- ggplot(LNR_sample, aes(RT, color = Choice, linetype = Won)) +\n  stat_function(fun = LNR_true,\n                aes(color = \"Correct\"),\n                linewidth = 0.8) +\n  stat_function(fun = LNR_false,\n                aes(color = \"Incorrect\"),\n                linewidth = 0.8) +\n  geom_segment(aes(x = RT, xend = RT), y = -0.3, yend = 0) + \n  geom_segment(x = 0, xend = 2.5, y = 0, yend = 0, color = \"black\",\n               arrow = arrow(length = unit(4, \"mm\"), type = \"open\")) +\n  geom_segment(x = 0, xend = 0.4, y = -0.1, yend = -0.1, color = \"black\") +\n  geom_segment(x = 0, xend = 0, y = -0.07, yend = -0.13, color = \"black\") +\n  geom_segment(x = 0.4, xend = 0.4, y = -0.07, yend = -0.13, color = \"black\") +\n  annotate(\"text\", x = 0.2, y = -0.2, label = \"t[0]\", parse = T) +\n  xlim(0, 2.5) +\n  scale_y_continuous(limits = c(-0.3, 1.5), expand = c(0,0)) +\n  ggtitle(\"LNR\") + \n  theme_classic() +\n  theme(axis.text = element_blank(),\n        axis.title.y = element_blank(),\n        axis.ticks = element_blank(),\n        axis.line.y = element_blank(),\n        axis.line.x = element_blank()) +\n  labs(y = \"Response Time\") +\n  scale_color_manual(values = rev(ft_colors)) +\n  guides(linetype = \"none\") +\n  xlab(\"Response Time\")\n\n(LBA_plot | RDM_plot) / free(LNR_plot)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning in stat_function(fun = dLBA, aes(color = \"True\"), args = list(response = 2, : All aesthetics have length 1, but the data has 3 rows.\ni Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning in stat_function(fun = dLBA, aes(color = \"False\"), args = list(response = 1, : All aesthetics have length 1, but the data has 3 rows.\ni Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nResults based on 2 accumulators/drift rates.\nResults based on 2 accumulators/drift rates.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning in geom_segment(aes(x = 0.2, y = 1, xend = (4 - 1)/5, yend = 4, : All aesthetics have length 1, but the data has 3 rows.\ni Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning in geom_segment(aes(x = 0.2, y = 1.3, xend = (4 - 1.3)/2.5, yend = 4, : All aesthetics have length 1, but the data has 3 rows.\ni Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning in stat_function(fun = gRDM, args = list(v1 = drift_true, v2 = drift_false, : All aesthetics have length 1, but the data has 7 rows.\ni Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning in stat_function(fun = gRDM, args = list(v1 = drift_false, v2 = drift_true, : All aesthetics have length 1, but the data has 7 rows.\ni Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning: Multiple drawing groups in `geom_function()`\ni Did you use the correct group, colour, or fill aesthetics?\nMultiple drawing groups in `geom_function()`\ni Did you use the correct group, colour, or fill aesthetics?\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning: Removed 7 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Simple Race Model Illustrations](index_files/figure-pdf/fig-racemodels-1.pdf){#fig-racemodels fig-pos='H'}\n:::\n:::\n\n\n\n\nEach of these race models have defined probability density functions $p(t \\mid \\boldsymbol{\\theta})$ and cumulative density functions $P(t \\mid \\boldsymbol{\\theta})$ for the finishing time of a single accumulator. To compute the likelihood of parameter vector $\\boldsymbol{\\theta}$ with winning accumulator $i$ at time $t$, we can take the probability density for the winning accumulator $p(t \\mid \\boldsymbol{\\theta}_{i})$, and multiply it by the probability that none of the other accumulators $j$ finish before time $t$:\n$$\n\\mathcal{L}(\\boldsymbol{\\theta} \\mid t, i) = p(t \\mid \\boldsymbol{\\theta}_{i}) \\prod_{j \\neq i}{1 - P(t \\mid \\boldsymbol{\\theta}_{j})},\n$$ {#eq-race-likelihood}\nwhich is the \"defective distribution\" for $i$ when written as a function of $t$, meaning that it integrates to the probability of response $i$.\n\nComplicating the estimation of speeded decision making models, RTs and choices are often missing on certain trials by design. A researcher may want to limit slow RTs in their experiment design, for example to reduce slow type II thinking [@dualprocess], or to emphasise speed. Alternatively, researchers may want to remove outlying responses that cannot have come from the process of interest (e.g., a response 0.05 seconds after stimulus onset, which is too fast for a decision making process to occur).\n\nThere are two main ways to handle missing values: truncation, which discards missing values with no assumption of the underlying distribution, and censoring, which assumes the proportion of missing values to reflect the true distribution, and takes this into account in the model estimation.\n\nAlthough truncation could potentially improve parameter estimates by eliminating irrelevant outliers, outlier removal often increases estimation bias by excluding extreme but valid RTs [@miller; @ulrichmiller; @outliersRatcliff; @MLcensoring]. Even less extreme values might be truncated in experiments with short time windows, leading to even worse estimates. @fig-LBAtrunc illustrates how upper truncation might distort parameter estimation in a simple two forced choice decision task with slow errors. Trials with slower accumulation rates---which tend to result in more incorrect trials---get discarded, while trials with higher accumulation rates are used to estimate the underlying parameters.\n\n\n\n\n::: {#cell-fig-LBAtrunc .cell apa-note='This figure illustrates how upper censoring and truncation relate to the LBA and the defective density. Dashed lines represent the cognitive dynamics behind missing RT trials and the missing tails of the defective densities. Truncation discards the corresponding data completely, while censoring uses the shaded areas under the defective density curves when computing the likelihood of the parameter estimates.'}\n\n```{.r .cell-code .hidden}\nLT = 0\nUT = 1\nl = 0.8\n\nntrials <- 40\n\nset.seed(4)\nslopes1 <- rnorm(ntrials,3,1)\nintercepts1 <- runif(ntrials,0,2)\nboundaryx1 <- (4 - intercepts1) / slopes1\n\nslopes2 <- rnorm(ntrials,4,1.75)\nintercepts2 <- runif(ntrials,0,2)\nboundaryx2 <- (4 - intercepts2) / slopes2\n\nwon <- boundaryx1 < boundaryx2\nlate1 <- boundaryx1 > UT - 0.2\nlate2 <- boundaryx2 > UT - 0.2\n\naccumulators <- data.frame(slopes1, intercepts1, boundaryx1,\n                           slopes2, intercepts2, boundaryx2,\n                           won, late1, late2)\n\nLBA_densities <- ggplot(data.frame(rt = seq(0, 2, length.out = 1000)), aes(rt)) +\n  stat_function(fun = dLBA,\n                aes(color = \"Incorrect\"),\n                args = list(response = 1, A=2, b= 4, t0 = .2, mean_v=c(3,4), sd_v=c(1,1.75)),\n                xlim = c(0, UT+0.0001),\n                n = 1001,\n                linewidth = l) +\n  stat_function(fun = dLBA,\n                args = list(response = 2, A=2, b= 4, t0 = .2, mean_v=c(3,4), sd_v=c(1,1.75)),\n                xlim = c(0, UT+0.0001),\n                n = 1001,\n                aes(color = \"Correct\"),\n                linewidth = l) +\n  stat_function(fun = dLBA,\n                aes(color = \"Incorrect\"),\n                args = list(response = 1, A=2, b=4, t0 = .2, mean_v=c(3,4), sd_v=c(1,1.75)),\n                xlim = c(UT, 2),\n                linetype = \"dashed\",\n                linewidth = l) +\n  stat_function(fun = dLBA,\n                args = list(response = 2, A=2, b=4, t0 = .2, mean_v=c(3,4), sd_v=c(1,1.75)),\n                xlim = c(UT, 2),\n                aes(color = \"Correct\"),\n                linetype = \"dashed\",\n                linewidth = l) +\n  stat_function(fun = dLBA,\n                args = list(response = 1, A=2, b=4, t0 = .2, mean_v=c(3,4), sd_v=c(1,1.75)),\n                xlim = c(UT, 3),\n                geom = \"area\",\n                aes(fill = \"Incorrect\"),\n                alpha = 0.25) +\n  stat_function(fun = dLBA,\n                args = list(response = 2, A=2, b=4, t0 = .2, mean_v=c(3,4), sd_v=c(1,1.75)),\n                xlim = c(UT, 2),\n                geom = \"area\",\n                aes(fill = \"Correct\"),\n                alpha = 0.25) +\n  geom_vline(xintercept = UT) + \n  theme_classic() +\n  theme(axis.ticks = element_blank(),\n        axis.text = element_blank(),\n        axis.title.x = element_blank(),\n        plot.margin = margin(b = 0),\n        axis.ticks.length = unit(0, \"pt\"),\n        axis.line.y = element_blank(),\n        axis.title.y = element_blank()) +\n  scale_x_continuous(limits=c(0, 2), expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  guides(fill = \"none\") +\n  labs(color = \"Choice\") +\n  scale_color_manual(values = rev(ft_colors)) +\n  scale_fill_manual(values = rev(ft_colors))\n\nLBA_racers <- ggplot(data.frame(rt = seq(0, 2, length.out = 1000)), aes(rt)) +\n  geom_segment(data = accumulators[won,],\n               aes(x = 0.2, y = intercepts1, xend = boundaryx1 + 0.2, yend = 4,\n                   linetype = late1, colour = \"Incorrect\")) +\n  geom_segment(data = accumulators[!won,],\n               aes(x = 0.2, y = intercepts2, xend = boundaryx2 + 0.2, yend = 4,\n                   linetype = late2, colour = \"Correct\")) +\n  scale_x_continuous(limits=c(0, 2), expand = c(0, 0)) +\n  scale_y_continuous(limits=c(0, 4), expand = c(0, 0)) +\n  theme_classic() +\n  geom_hline(yintercept = 4, linewidth = 2) +\n  geom_vline(xintercept = UT) +\n  geom_vline(xintercept = 0.001,\n             linetype = \"dashed\") +\n  geom_vline(xintercept = 0.2,\n             linetype = \"dashed\") +\n  theme(plot.margin = margin(t = 0),\n        axis.text = element_blank(),\n        axis.title.y = element_blank(),\n        axis.ticks = element_blank(),\n        axis.line.y = element_blank(),\n        axis.line.x = element_line(arrow = arrow(length = unit(4, \"mm\"),\n                                                 type = \"open\")),\n        legend.position = \"none\") +\n  xlab(\"Response Time\") +\n  geom_segment(x = 0.01, xend = 0.19, y = 3.8, yend = 3.8, linewidth = 0.6) +\n  geom_segment(x = 0.01, xend = 0.01, y = 3.7, yend = 3.9) +\n  geom_segment(x = 0.19, xend = 0.19, y = 3.7, yend = 3.9) +\n  annotate(\"text\", x = 0.1, y  = 3.6, label = \"t[0]\", parse = TRUE) +\n  annotate(\"rect\", xmin = 0.1, xmax = 0.2, ymin = 0, ymax = 2, alpha = 0.2) +\n  annotate(\"text\", x = 0.15, y = 2.2, label = \"A\") +\n  geom_segment(x = 1.8, xend = 1.8, y = 0, yend = 4, \n               arrow = arrow(length = unit(3, \"mm\"), type = \"open\")) +\n  annotate(\"text\", x = 1.83, y = 2.5, label = \"b\") +\n  scale_color_manual(values = rev(ft_colors))\n\nLBA_densities / LBA_racers\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nResults based on 2 accumulators/drift rates.\nResults based on 2 accumulators/drift rates.\nResults based on 2 accumulators/drift rates.\nResults based on 2 accumulators/drift rates.\nResults based on 2 accumulators/drift rates.\nResults based on 2 accumulators/drift rates.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Illustration of Missing Upper Response Times in the Linear Ballistic Accumulator](index_files/figure-pdf/fig-LBAtrunc-1.pdf){#fig-LBAtrunc fig-pos='H'}\n:::\n:::\n\n\n\n\nDespite the issues, researchers often use truncation for missing RTs, perhaps because of common practice, or because computing the likelihood for a censored RT requires computing the area under the curve of the likelihood function over the censored range. For race models, this means that we integrate @eq-race-likelihood over the censored time range $R$:\n$$\n\\mathcal{L}(\\boldsymbol{\\theta} \\mid t \\in R, i) = \\int_{R} p(t \\mid \\boldsymbol{\\theta}_{i}) \\prod_{j \\neq i}{1 - P(t \\mid \\boldsymbol{\\theta}_{j})} \\, dt.\n$$ {#eq-censoring}\nSince censoring in maximum likelihood estimation and Markov chain Monte Carlo methods require numerically computing this integral for many iterations, censoring is often slow. \n\n@eq-censoring can be extended to accomodate missing responses in addition to missing RTs by summing @eq-censoring for each accumulator, resulting in the total probability of any response in time range $R$. Similarly, if we censored both fast and slow responses with no distinction between the two, we can sum the integrals over the lower and the upper range. We can even combine censoring and truncation with \n$$\n\\mathcal{L}(\\boldsymbol{\\theta} \\mid t \\in R, i) \\frac{\\mathcal{L}(\\boldsymbol{\\theta} \\mid 0 \\leq t < \\infty, i)}{\\mathcal{L}(\\boldsymbol{\\theta} \\mid t \\in S, i)},\n$$ {#eq-censoringandtruncation}\nwhere $S$ is the range of all untruncated values.\n\nAlthough censoring has been shown to result in better parameter recovery than truncation in common RT distributions [@MLcensoring], a difference in parameter recovery has not yet been established for race models. As data are routinely censored or truncated, this study compares censoring and truncation on parameter recovery at different levels of missingness, for the LBA, LNR, and RDM. Parameter estimates are expected to deteriorate at a higher rate for truncation than for censoring, with increases in the proportion of missing RTs. \n\n# Methods\n\nWe compared censoring and truncation in two simulation studies. The first study compared upper censoring and truncation with the responses known with a large number of samples to assess asymptotic parameter identifiability. The second study assessed parameter recovery on a smaller number of trials to evaluate the practical differences between censoring and truncation, comparing a larger number of missing data scenarios.\n\nTo assess parameter identifiability, we first simulated data using known parameters, which we then fit the same model to. This allows us to compare the known, \"true\" parameter values with our fitted parameter values. The code for all analyses and data generation are available on https://github.com/timmerj1/censoring-truncation-study-EAMs.\n\nFor both simulation studies, a simple model with two stimuli and two racers was used to generate the data. Conventional constants for the LBA's ($s_v = 1$) and the RDM's drift rate variance ($s = 1$) were used to generate and fit the data. Parameter values were chosen to reflect common RT and choice distributions in simple two forced choice tasks (see @tbl-pars for an overview of parameters used). For the LBA, decision thresholds were defined with $B = b - A$. As is the default in EMC2, parameters on the positive real line were log transformed. Data was generated using the `make_data` function from EMC2 [@EMC2].\n\n\n\n\n::: {#tbl-pars .cell tbl-cap='Race Model Simulation Parameters' apa-note='The parameters that were used to simulate the response time and choice data. Parameters on the real positive line are estimated on a log scale in EMC2. Subscript \\'true\\' relates to a match between stimulus and choice (a correct choice is made), and these parameters are added to the corresponding parameter for the non-matching racer.' ft-align='left'}\n\n```{.r .cell-code .hidden}\nparams <- data.frame(\n  LBA_Param = c(\"$B$\", \"$v$\", \"$v_{true}$\", \"$A$\", \"$s_{v_{true}}$\", \"$B$\"),\n  LBA_Value = c(2, 3, 1, 2, 0.75, \"2\"),\n  LNR_Param = c(\"$m$\", \"$m_{true}$\", \"$s$\", \"$s_{true}$\", \"$t_0$\", \"\"),\n  LNR_Value = c(0.75, 0.65, 0.5, 0.8, 0.4, \"\"),\n  RDM_Param = c(\"$B$\", \"$v$\", \"$v_{true}$\", \"$s_{true}$\", \"$t_0$\", \"\"),\n  RDM_Value = c(3, 1, 4, 0.75, 0.2, \"\")\n)\n\nkbl(params, escape = FALSE, col.names = NULL, booktabs = TRUE) %>%\n  add_header_above(rep(c(\"Parameter\", \"Value\"), 3)) %>%\n  add_header_above(c(\"LBA\" = 2, \"LNR\" = 2, \"RDM\" = 2))\n```\n\n::: {.cell-output-display}\n\n\\begin{tabular}[t]{llllll}\n\\toprule\n\\multicolumn{2}{c}{LBA} & \\multicolumn{2}{c}{LNR} & \\multicolumn{2}{c}{RDM} \\\\\n\\cmidrule(l{3pt}r{3pt}){1-2} \\cmidrule(l{3pt}r{3pt}){3-4} \\cmidrule(l{3pt}r{3pt}){5-6}\n\\multicolumn{1}{c}{Parameter} & \\multicolumn{1}{c}{Value} & \\multicolumn{1}{c}{Parameter} & \\multicolumn{1}{c}{Value} & \\multicolumn{1}{c}{Parameter} & \\multicolumn{1}{c}{Value} \\\\\n\\cmidrule(l{3pt}r{3pt}){1-1} \\cmidrule(l{3pt}r{3pt}){2-2} \\cmidrule(l{3pt}r{3pt}){3-3} \\cmidrule(l{3pt}r{3pt}){4-4} \\cmidrule(l{3pt}r{3pt}){5-5} \\cmidrule(l{3pt}r{3pt}){6-6}\n$B$ & 2 & $m$ & 0.75 & $B$ & 3\\\\\n$v$ & 3 & $m_{true}$ & 0.65 & $v$ & 1\\\\\n$v_{true}$ & 1 & $s$ & 0.5 & $v_{true}$ & 4\\\\\n$A$ & 2 & $s_{true}$ & 0.8 & $s_{true}$ & 0.75\\\\\n$s_{v_{true}}$ & 0.75 & $t_0$ & 0.4 & $t_0$ & 0.2\\\\\n\\addlinespace\n$B$ & 2 &  &  &  & \\\\\n\\bottomrule\n\\end{tabular}\n\n\n:::\n:::\n\n\n\n\nFor the first simulation, upper censoring and truncation were compared using a large number of trials (20,000 trials, 10,000 per stimulus) to investigate the differences between censoring and truncation without fits being affected by random error. RTs were cut off at three different levels: at 2.5%, 10%, and 30% of upper values, with cutoff points estimated in a separate simulation with the same number of trials. Responses were not missing for any of the censored values. For each combination of model and missing level, new data was simulated, but censoring and truncation were compared on the same datasets to ensure differences cannot be caused by random sampling error.\n\nIn the second simulation, a factorial design was used to vary whether responses were known or unknown, which tail missed response times (lower, upper, or both tails), the percentage of missing responses (2%, 10%, 30%, or 50%), and whether missing responses were censored or truncated. For each condition, ten samples with a small number of trials (400 trials) were simulated and fit. Missing level cutoffs were chosen using the quantiles on a non-missing simulation with 4000 trials.\n\nParameter posteriors were sampled using the particle Metropolis within Gibbs [PMwG\\; @PMwG] sampler in EMC2 [@EMC2] using its default (standard normal) priors for non-hierarchical estimation. Three PMwG chains were sampled for each parameter, with 50 particles per parameter [@PMwG].\n\nTo assess parameter recovery, we used the Root Mean Squared Errors (RMSEs) between the posterior medians $\\boldsymbol{\\hat{\\theta}}$ and the true parameters $\\boldsymbol{\\theta}$: \n$$\nRMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}{(\\hat{\\theta}_i - \\theta_i)^2}}.\n$$ {#eq-RMSE}\n\nSince we used Bayesian methods, the parameter estimates are not restricted to point estimates. We will compute the quadratic Wasserstein distance $W_2$ between the full posterior $P$ samples $X_1,...,X_{n_P}$ and Dirac point mass distribution $\\delta_{\\boldsymbol{\\theta}}$ at the true parameters $\\boldsymbol{\\theta}$:\n$$\nW_2(\\delta_{\\boldsymbol{\\theta}}, P) = \\sqrt{\\frac{1}{n_P} \\sum_{i=1}^{n_P}{\\|X_i - \\boldsymbol{\\theta}\\|^2}}.\n$$ {#eq-Wasserstein}\nThe double vertical bars indicate the distance in vector space, as the Wasserstein distance can be taken for the full model using the Euclidean distance, and for the separate parameters with unidimensional distance (since the difference is squared, no vertical bars are needed). \n\nThe quadratic Wasserstein distance is affected by both the position and the scale of the posterior. The quadratic Wasserstein over a single dimension becomes an unbiased estimate of the standard deviation of $P$ when $\\theta$ is the mean of $P$, and it simplifies to the RMSE when $P$ is at a single point. This makes the quadratic Wasserstein distance an interesting Bayesian alternative to the RMSE, measuring the distance between the full posterior and the true parameter.\n\nFor completeness, additional Pearson's correlation coefficients and mean absolute errors were computed, but since these did not differ substantially from the RMSE and $W_2$, these statistics were plotted in the appendix. Parameters that were log transformed to be estimated on the real scale were not transformed back when computing these statistics.\n\n# Results\n\n# Study 1\n\n@fig-upper shows the RMSE between the true parameter values and the medians of the posterior distribution for each fit. As expected, both distance measures RMSE and $W_2$ increased with an increase of missing RTs for truncation. For censoring, the distance measures stay closer to 0 and do not clearly increase when data is censored rather than truncated. This indicates that censoring improved the parameter recovery in an asymptotic fit as expected. The only case where censoring does not seem to outperform truncation is for the RDM, where 2.5% truncation did not perform worse than 2.5% censoring. With higher missing percentages, censoring still outperformed truncation for the RDM.\n\n\n\n\n::: {#cell-fig-upper .cell apa-note='For each model and each level of missingness, the root mean squared errors (RMSE) and quadratic Wasserstein distances ($W_2$) were computed and plotted. Lower RMSE and $W_2$ indicate better parameter recovery.'}\n\n```{.r .cell-code .hidden}\nRMSE_upper_plot <- EMCs_upper %>%\n    ggplot(aes(Missing, RMSE, fill = Censoring, alpha = Missing)) +\n    facet_nested( ~ Model + Censoring, switch = \"x\",\n                  strip = strip_split(position = c(\"top\", \"bottom\"))) +\n    geom_bar(stat = \"identity\") +\n    labs(fill = \"Method\") +\n    theme_classic() +\n    scale_fill_manual(values = ct_colors) +\n    scale_alpha_ordinal(range = c(1, 0.4)) +\n    theme(strip.placement = \"outside\",\n          strip.background = element_blank(),\n          axis.title.x = element_blank(),\n          legend.position = \"none\",\n          strip.text.x.top = element_text(face = \"bold\"))\n\nWasserstein_upper_plot <- Wasserstein_upper %>%\n    ggplot(aes(Missing, W, fill = Censoring, alpha = Missing)) +\n    facet_nested( ~ Model + Censoring, switch = \"x\",\n                  strip = strip_split(position = c(\"top\", \"bottom\"))) +\n    geom_bar(stat = \"identity\") +\n    labs(fill = \"Method\") +\n    theme_classic() +\n    scale_fill_manual(values = ct_colors) +\n    scale_alpha_ordinal(range = c(1, 0.4)) +\n    theme(strip.placement = \"outside\",\n          strip.background = element_blank(),\n          axis.title.x = element_blank(),\n          legend.position = \"none\",\n          strip.text.x.top = element_text(face = \"bold\"))\n\nRMSE_upper_plot / Wasserstein_upper_plot\n```\n\n::: {.cell-output-display}\n![RMSE and $W_2$ for Upper Censoring](index_files/figure-pdf/fig-upper-1.pdf){#fig-upper fig-pos='H'}\n:::\n:::\n\n\n\n\nLooking at the credible intervals for each parameter in @fig-upper-CI, the generally higher RMSE for RDMs is explained by a general tendency for $B$ and $v$ to be overestimated, while $v_{win}$ and $t_0$ are underestimated. Overall, these parameters still seem to be recovered better with censoring than with truncation, except that $t_0$ and $s_{win}$ were recovered slightly better for truncation at a low percentage in this simulation. For the other models, censoring clearly performs better than truncation, with true parameters included by most credible intervals for censoring, while most truncation credible intervals exclude the true parameters. The main exception to this is the $s_{win}$ parameter for the LNR, where the credible interval for 30% censoring does not include the true parameter value.\n\n\n\n\n::: {#cell-fig-upper-CI .cell apa-note='Race model recoveries for the linear ballistic accumulator model (LBA), the log-normal race model (LNR) and the racing diffusion model (RDM) with 2%, 10%, or 30% missing. Dots are placed at the posterior median, and error bars denote the 95% equal-tailed credible intervals of the parameter posteriors. The dashed vertical lines represent the true parameter values.'}\n\n```{.r .cell-code .hidden}\nLBA_upper_CI <- parameters_CI_upper %>%\n  filter(Model == \"LBA\") %>%\n  ggplot(aes(`50%`, Missing, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = `2.5%`, xmax = `97.5%`)) +\n  facet_grid(Censoring ~ Parameter, scale = \"free_x\",\n             labeller = labeller(.default = label_parsed,\n                                 Parameter = parlabeller),\n             switch = \"y\") +\n  xlab(\"Parameter Estimates\") + \n  ylab(\"\") +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() +\n  ggtitle(\"LBA\") +\n  scale_y_discrete(limits=rev) +\n  scale_color_manual(values = rev(ct_colors)) +\n  theme(strip.text.y.left = element_text(angle = 0),\n        strip.background.y = element_blank(),\n        strip.placement = \"outside\",\n        legend.position = \"none\",\n        panel.spacing.y = unit(0, \"lines\"),\n        plot.margin = margin(l = 0, b = 10),\n        axis.text.x = element_text(size = 8)) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n  \n\nLNR_upper_CI <- parameters_CI_upper %>%\n  filter(Model == \"LNR\") %>%\n  ggplot(aes(`50%`, Missing, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = `2.5%`, xmax = `97.5%`)) +\n  facet_grid(Censoring ~ Parameter, scale = \"free_x\",\n             labeller = labeller(.default = label_parsed,\n                                 Parameter = parlabeller),\n             switch = \"y\") +\n  xlab(\"Parameter Estimates\") + \n  ylab(\"\") +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() +\n  ggtitle(\"LNR\") +\n  scale_y_discrete(limits=rev) +\n  scale_color_manual(values = rev(ct_colors)) +\n  theme(strip.text.y.left = element_text(angle = 0),\n        strip.background.y = element_blank(),\n        strip.placement = \"outside\",\n        legend.position = \"none\",\n        panel.spacing.y = unit(0, \"lines\"),\n        plot.margin = margin(l = 0, b = 10),\n        axis.text.x = element_text(size = 8)) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n\nRDM_upper_CI <- parameters_CI_upper %>%\n  filter(Model == \"RDM\") %>%\n  ggplot(aes(`50%`, Missing, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = `2.5%`, xmax = `97.5%`)) +\n  facet_grid(Censoring ~ Parameter, scale = \"free_x\",\n             labeller = labeller(.default = label_parsed,\n                                 Parameter = parlabeller),\n             switch = \"y\") +\n  xlab(\"Parameter Estimates\") + \n  ylab(\"\") +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() +\n  ggtitle(\"RDM\") +\n  scale_y_discrete(limits=rev) +\n  scale_color_manual(values = rev(ct_colors)) +\n  theme(strip.text.y.left = element_text(angle = 0),\n        strip.background.y = element_blank(),\n        strip.placement = \"outside\",\n        legend.position = \"none\",\n        panel.spacing.y = unit(0, \"lines\"),\n        plot.margin = margin(l = 0, b = 10),\n        axis.text.x = element_text(size = 8)) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n\nLBA_upper_CI / LNR_upper_CI / RDM_upper_CI + plot_layout(axis_titles = \"collect\")\n```\n\n::: {.cell-output-display}\n![Parameter Recoveries and 95% CIs for Upper Censoring and Truncation](index_files/figure-pdf/fig-upper-CI-1.pdf){#fig-upper-CI fig-pos='H'}\n:::\n:::\n\n\n\n\nOverall, these results indicate that upper censoring results in better parameter recovery than upper truncation in an asymptotic sample, i.e. with a large number of trials. This cannot yet be generalized to smaller sample sizes, as random error and parameter tradeoffs might affect parameter recovery more than censoring versus truncation does. Moreover, one might want to use lower censoring or truncation instead, or a combination of lower and upper censoring or truncation. Lastly, in this simulation the responses were known. Often censoring or truncation is implemented when there is a response time window, where neither the RTs or the choices are recorded. To account for these issues, the second simulation study takes these factors into account by using a smaller number of trials (200 instead of 1000 trials), in a $2 \\times 2 \\times 3 \\times 3 \\times 4$ design: censoring versus truncation, known versus unknown, lower versus upper versus both tails missing, LBA versus LNR versus RDM, and 2%, 10%, 30%, and 50% censoring or truncation.\n\n# Study 2\n## Linear Ballistic Accumulator Model\nContrary to the first study, upper censoring with responses known did not have better parameter recovery than upper truncation for the LBA. @fig-LBA-model shows similar RMSEs and $W_2$s for all upper censoring and truncation, both getting worse with higher missing percentages. Contrary to this, fits for the lower tail clearly diverge with increasing missing percentages, with censoring outperforming truncation. Interestingly, upper tail censoring and truncation both seem to perform on par with lower tail censoring, with relatively low RMSE and $W_2$. Censoring on both tails resulted in similar RMSE and $W_2$ compared to truncation, but censored RMSEs and $W_2$s were generally lower.\n\n\n\n\n::: {#cell-fig-LBA-model .cell apa-note='Root mean squared errors (RMSE) and quadratic Wasserstein distances ($W_2$) over all parameters for the linear ballistic accumulator model (LBA). Colors denote whether missing RTs were censored or truncated. Points were randomly horizontally shifted to avoid overlap. Lower RMSE and $W_2$ indicate better parameter recovery.'}\n\n```{.r .cell-code .hidden}\nRMSE_plot <- EMCs_both_RMSEs %>%\n  filter(Model == \"LBA\") %>%\n  ggplot(aes(Missing, RMSE, color = Censoring, group = Censoring)) +\n  facet_grid(Response ~ Tail,\n             labeller = function(x) label_both(x, sep = \":\\n\")) +\n  geom_point(position = position_jitter(width = 0.3, height = 0),\n             alpha = 0.7) +\n  labs(color = \"\") +\n  theme_classic() +\n  scale_y_continuous(n.breaks = 3) +\n  scale_color_manual(values = ct_colors)\n\nW_plot <- Wasserstein_both %>%\n  filter(Model == \"LBA\" & Variable == \"model\") %>%\n  ggplot(aes(Missing, W, color = Censoring, group = Censoring)) +\n  facet_grid(Response ~ Tail,\n             labeller = function(x) label_both(x, sep = \":\\n\")) +\n  geom_point(position = position_jitter(width = 0.3, height = 0),\n             alpha = 0.7) +\n  labs(color = \"\") +\n  theme_classic() +\n  scale_color_manual(values = ct_colors) +\n  ylab(parse(text = \"W[2]\"))\n\nRMSE_plot / W_plot + plot_layout(axis_titles = \"collect\", guides = \"collect\")\n```\n\n::: {.cell-output-display}\n![Model RMSE and $W_2$ for the LBA](index_files/figure-pdf/fig-LBA-model-1.pdf){#fig-LBA-model fig-pos='H'}\n:::\n:::\n\n\n\n\n@fig-LBA-pars shows the $W_2$ for each LBA parameter. In accordance with @fig-LBA-model, lower censoring resulted in similar or lower $W_2$s than lower truncation. Especially the boundary $B$, non-decision time $t_0$, and mean evidence accumulation rate $v$ showed better parameter recovery for censoring compared to truncation when responses are unknown, while lower censoring with responses known improved parameter recovery for all parameters. With both tails missing, censoring showed better parameter recovery than truncation for most parameters, but worse recovery for starting point variability $A$ and mean evidence accumulation rate $v$, explaining the similar RMSE and $W_2$ in @fig-LBA-model. Lastly, when the upper tail was missing, truncation recovered the parameters similarly to censoring, with truncation showing better parameter recovery of starting point variability $A$.\n\n\n\n\n::: {#cell-fig-LBA-pars .cell apa-note='Quadratic Wasserstein distances ($W_2$) between the true linear ballistic accumulator (LBA) parameters and each parameter posterior for each dataset. Each LBA parameter is represented by a different plot row labeled on the right, while each combination of tail and response condition is represented by the columns. Colors denote whether missing RTs were censored or truncated. Points were randomly horizontally shifted to avoid overlap. Lower $W_2$ indicates better parameter recovery.'}\n\n```{.r .cell-code .hidden}\nWasserstein_both %>%\n  filter(Model == \"LBA\" & Variable != \"model\") %>%\n  ggplot(aes(Missing, W, color = Censoring, group = Censoring)) +\n  facet_nested(Variable ~ Response + Tail, scales = \"free_y\",\n               labeller = labeller(.default = label_parsed,\n                                   Response = label_both,\n                                   Tail = label_both,\n                                   Variable = parlabeller)) +\n  geom_point(position = position_jitter(width = 0.3, height = 0),\n             alpha = 0.7) +\n  labs(fill = \"Missing\",\n       color = \"\") +\n  theme_classic() +\n  scale_color_manual(values = ct_colors) +\n  ylab(parse(text = \"W[2]\")) +\n  theme(strip.text.y = element_text(angle = 0),\n        legend.position = \"bottom\",\n        axis.text.x = element_text(angle = 90))\n```\n\n::: {.cell-output-display}\n![$W_2$ by Parameter for the LBA](index_files/figure-pdf/fig-LBA-pars-1.pdf){#fig-LBA-pars fig-pos='H'}\n:::\n:::\n\n\n\n\n## Log-Normal Race Model\n\nFor the LNR, both upper and lower censoring showed better parameter recovery as expected. For the lower and upper tail simulation, @fig-LNR-model shows the expected pattern of increasing RMSEs and $W_2$s for truncation while censoring RMSEs and $W_2$s remain low. Unexpectedly, missing values in both tails did not result in the same clear pattern, especially for $W_2$. This difference between RMSEs and $W_2$ indicate that although the medians of censoring posterior distributions were closer to the true parameter values, the complete posteriors had similar distances to the true parameter values for censoring compared to truncation.\n\n\n\n\n::: {#cell-fig-LNR-model .cell apa-note='Root mean squared errors (RMSE) and quadratic Wasserstein distances ($W_2$) over all parameters for the log-normal race model (LNR). Colors denote whether missing RTs were censored or truncated. Points were randomly horizontally shifted to avoid overlap. Lower RMSE and $W_2$ indicate better parameter recovery.'}\n\n```{.r .cell-code .hidden}\nRMSE_plot <- EMCs_both_RMSEs %>%\n  filter(Model == \"LNR\") %>%\n  ggplot(aes(Missing, RMSE, color = Censoring, group = Censoring)) +\n  facet_grid(Response ~ Tail,\n             labeller = function(x) label_both(x, sep = \":\\n\")) +\n  geom_point(position = position_jitter(width = 0.3, height = 0),\n             alpha = 0.7) +\n  labs(fill = \"Missing\") +\n  theme_classic() +\n  scale_y_continuous(n.breaks = 3) +\n  scale_color_manual(values = ct_colors)\n\nW_plot <- Wasserstein_both %>%\n  filter(Model == \"LNR\" & Variable == \"model\") %>%\n  ggplot(aes(Missing, W, color = Censoring, group = Censoring)) +\n  facet_grid(Response ~ Tail,\n             labeller = function(x) label_both(x, sep = \":\\n\")) +\n  geom_point(position = position_jitter(width = 0.3, height = 0),\n             alpha = 0.7) +\n  labs(fill = \"Missing\") +\n  theme_classic() +\n  scale_color_manual(values = ct_colors) +\n  ylab(parse(text = \"W[2]\"))\n\nRMSE_plot / W_plot + plot_layout(axis_titles = \"collect\", guides = \"collect\")\n```\n\n::: {.cell-output-display}\n![Model RMSE and $W_2$ for the LNR](index_files/figure-pdf/fig-LNR-model-1.pdf){#fig-LNR-model fig-pos='H'}\n:::\n:::\n\n\n\n\nLooking at the parameter $W_2$s in @fig-LNR-pars, we see that lower truncation particularly deteriorated the estimates for the log-normal $\\mu$ estimates $m$ and $m_{true}$, and non-decision time $t_0$ compared to lower censoring, whereas upper truncation deteriorated the estimates for log-normal $\\sigma$ estimate $s$ and $t_0$ compared to upper censoring. Surprisingly, $W_2$s for $t_0$ estimates were worse for upper truncation than lower truncation, even though non-decision time is largely reflected by the minimum RT. When responses were unknown, lower and two-tailed censoring had worse parameter recovery for $s_{true}$ than truncation. Two-tailed censoring recovered $m$ and $s$ better than two-tailed truncation, but resulted in recoveries that were similar to truncation for the other parameters.\n\n\n\n\n::: {#cell-fig-LNR-pars .cell apa-note='Quadratic Wasserstein distances ($W_2$) between the true log-normal race (LNR) parameters and each parameter posterior for each dataset. Each LNR parameter is represented by a different plot row labeled on the right, while each combination of tail and response condition is represented by the columns. Colors denote whether missing RTs were censored or truncated. Points were randomly horizontally shifted to avoid overlap. Lower $W_2$ indicates better parameter recovery.'}\n\n```{.r .cell-code .hidden}\nWasserstein_both %>%\n  filter(Model == \"LNR\" & Variable != \"model\") %>%\n  ggplot(aes(Missing, W, color = Censoring, group = Censoring)) +\n  facet_nested(Variable ~ Response + Tail, scales = \"free_y\",\n               labeller = labeller(.default = label_parsed,\n                                   Response = label_both,\n                                   Tail = label_both,\n                                   Variable = parlabeller)) +\n  geom_point(position = position_jitter(width = 0.3, height = 0),\n             alpha = 0.7) +\n  labs(fill = \"Missing\",\n       color = \"\") +\n  theme_classic() +\n  scale_color_manual(values = ct_colors) +\n  ylab(parse(text = \"W[2]\")) +\n  theme(strip.text.y = element_text(angle = 0),\n        legend.position = \"bottom\",\n        axis.text.x = element_text(angle = 90))\n```\n\n::: {.cell-output-display}\n![$W_2$ by Parameter for the LNR](index_files/figure-pdf/fig-LNR-pars-1.pdf){#fig-LNR-pars fig-pos='H'}\n:::\n:::\n\n\n\n\n## Racing Diffusion Model\n\nLike the LBA, @fig-RDM-model shows higher RMSEs and $W_2$s for lower truncation compared to lower censoring, while upper and two-tailed censoring show similar, lower RMSEs and $W_2$s compared to truncation. Only when responses were known, upper tail censoring resulted in lower RMSE and $W_2$ than truncation, but without a clear separation.\n\n\n\n\n::: {#cell-fig-RDM-model .cell apa-note='Root mean squared errors (RMSE) and quadratic Wasserstein distances ($W_2$) over all parameters for the racing diffusion model (RDM). Colors denote whether the missing RTs were censored or truncated. Points were randomly horizontally shifted to avoid overlap. Lower RMSE and $W_2$ indicate better parameter recovery.'}\n\n```{.r .cell-code .hidden}\nRMSE_plot <- EMCs_both_RMSEs %>%\n  filter(Model == \"RDM\") %>%\n  ggplot(aes(Missing, RMSE, color = Censoring, group = Censoring)) +\n  facet_grid(Response ~ Tail,\n             labeller = function(x) label_both(x, sep = \":\\n\")) +\n  geom_point(position = position_jitter(width = 0.3, height = 0),\n             alpha = 0.7) +\n  labs(fill = \"Missing\") +\n  theme_classic() +\n  scale_y_continuous(n.breaks = 3) +\n  scale_color_manual(values = ct_colors)\n\nW_plot <- Wasserstein_both %>%\n  filter(Model == \"RDM\" & Variable == \"model\") %>%\n  ggplot(aes(Missing, W, color = Censoring, group = Censoring)) +\n  facet_grid(Response ~ Tail,\n             labeller = function(x) label_both(x, sep = \":\\n\")) +\n  geom_point(position = position_jitter(width = 0.3, height = 0),\n             alpha = 0.7) +\n  labs(fill = \"Missing\") +\n  theme_classic() +\n  scale_color_manual(values = ct_colors) +\n  ylab(parse(text = \"W[2]\"))\n\nRMSE_plot / W_plot + plot_layout(axis_titles = \"collect\", guides = \"collect\")\n```\n\n::: {.cell-output-display}\n![Model RMSE and $W_2$ for the RDM](index_files/figure-pdf/fig-RDM-model-1.pdf){#fig-RDM-model fig-pos='H'}\n:::\n:::\n\n\n\n\nThe parameter $W_2$s shown in @fig-RDM-pars also showed a similar pattern to the LBA. Lower tail truncation mainly affected the boundary parameter $B$, the non-decision time $t_0$, and the drift rates $v$ and $v_{true}$ compared to lower censoring. The $W^2$s for the drift rate of the incorrect option, $v$, was increasing for truncation compared to censoring, regardless of the tail or whether responses were known. For the other parameters, upper tail and two-tailed censoring was not particularly better or worse than truncation, with an exception of the recovery of $B$ and $v_{true}$ for upper tail censoring with responses known.\n\n\n\n\n::: {#cell-fig-RDM-pars .cell apa-note='Quadratic Wasserstein distances ($W_2$) between the true racing diffusion model (RDM) parameters and each parameter posterior for each dataset. Each RDM parameter is represented by a different plot row labeled on the right, while each combination of tail and response condition is represented by the columns. Colors denote whether missing RTs were censored or truncated. Points were randomly horizontally shifted to avoid overlap. Lower $W_2$ indicates better parameter recovery.'}\n\n```{.r .cell-code .hidden}\nWasserstein_both %>%\n  filter(Model == \"RDM\" & Variable != \"model\") %>%\n  ggplot(aes(Missing, W, color = Censoring, group = Censoring)) +\n  facet_nested(Variable ~ Response + Tail, scales = \"free_y\",\n               labeller = labeller(.default = label_parsed,\n                                   Response = label_both,\n                                   Tail = label_both,\n                                   Variable = parlabeller)) +\n  geom_point(position = position_jitter(width = 0.3, height = 0),\n             alpha = 0.7) +\n  labs(fill = \"Missing\",\n       color = \"\") +\n  theme_classic() +\n  scale_color_manual(values = ct_colors) +\n  ylab(parse(text = \"W[2]\")) +\n  theme(strip.text.y = element_text(angle = 0),\n        legend.position = \"bottom\",\n        axis.text.x = element_text(angle = 90))\n```\n\n::: {.cell-output-display}\n![$W_2$ by Parameter for the RDM](index_files/figure-pdf/fig-RDM-pars-1.pdf){#fig-RDM-pars fig-pos='H'}\n:::\n:::\n\n\n\n\n# Discussion\n\nWe compared censoring and truncation for three different race models: the linear ballistic accumulator model, the log-normal race model, and the racing diffusion model. To this end, we simulated data using pre-specified parameter values, with missing data cutoffs based on quantiles. The first simulation study compared asymptotic parameter recovery upper censoring and truncation with responses known. The second simulation used several smaller datasets for each condition in a factorial design. It compared lower, upper and two-tailed censoring or truncation, with either results known or unknown.\n\nIn the first simulation, parameters were recovered well for censoring, while parameters estimated with truncation were considerably off, even at lower percentages of missingness. Only for the RDM, 2.5% truncation was on par with censoring. The RDM had worse parameter recovery than the other two models in general, with credible intervals for some parameters missing the true parameter for each level of censoring or truncation. Censoring still outperformed truncation in the RDM for higher percentages of missing RTs.\n\nThe second simulation showed that the previous results did not hold with smaller sample sizes for the LBA and RDM. However, lower truncation did result in worse parameter recovery than censoring. For the LNR, upper and lower truncation both resulted in better recovery than truncation, but two-tailed truncation was closer to censoring in terms of parameter recovery.\n\nThe results indicate that censoring asymptotically improves parameter recovery. In smaller numbers of trials, however, differences between censoring and truncation can become inconsequential compared to random sampling error. \n\nSince parameter recovery for censoring was not worse than for truncation, censoring should be preferred if parameter recovery is the only consideration. When computation time and resources are relevant factors, however, censoring might not always be worth the cost. When responses were unknown, resulting in a higher number of numerical integrals for each likelihood calculation, the PMwG sampler took more than 28 hours in an extreme case in the second simulation. In less extreme cases with responses known, this time was closer to 15 minutes.\n\nTo speed up the MCMC sampling for censoring, the likelihood function for censoring was translated from R to Rcpp. This approximately doubled the speed. In the Rcpp likelihood function, numerical integration was implemented with RcppNumerical @RcppNumerical, an Rcpp library for numerical integration and optimization. Although this results in faster likelihood estimations than with the `integrate` function in R, the computation time is still considerable. To make censoring more worthwhile, changing to a faster quadrature rule that does not have to work for complicated multimodal functions might be beneficial.\n\nThe simulations in this paper all used non-hierarchical models. Since the second simulation seems to have been more affected by random error than by truncation, future simulations might use hierarchical modeling, which is less affected by random error for single participants, but instead shrinks more extreme values to group level means [@shrinkage], making the inference more robust against random sampling error. Hierarchical models are increasingly used and it remains unclear how censoring and truncation affects hierarchical race model estimation.\n\nFuture studies might also investigate how censoring and truncation compare to baseline race model fits, as this study only focused on the comparison between censoring and truncation. Furthermore, simulation-based calibration [@SBC] might be used to check whether race model estimation using censoring and truncation appropriately reflect the uncertainty of the posterior.\n\nAnother avenue that remains uninvestigated is how contamination ties into censoring and truncation. Since truncation can remove outliers, truncation could potentially lead to better parameter recovery than censoring would in the presence of contaminant RTs. On the other hand, MCMC methods also allow for the explicit modeling of contaminants. The likelihood function that was implemented in EMC2 allows for combinations of censoring, truncation, and explicit contaminant modeling, and future simulations could investigate how combinations of these methods might lead to better parameter recovery.\n\nHow researchers handle outliers and missing data can greatly affect the outcomes of their research. Our results suggest that ignoring values outside of a prespecified response window or outside of common outlier thresholds can bias race model estimates. Although censoring can be computationally costly, it should be the default over truncation when the objective is accurate estimation of latent cognitive parameters.\n\n\n\n\n{{< pagebreak >}}\n\n\n\n\n\n\n# References\n\n::: {#refs}\n:::\n\n# Supplementary Materials {#apx-1}\n## Study 1\n\n\n\n\n::: {#cell-fig-upper-MAE .cell apa-note='For each model and each level of missingness, the mean absolute error (MAE) is plotted. Lower MAE indicates better parameter recovery.'}\n\n```{.r .cell-code .hidden}\nEMCs_upper %>%\n    ggplot(aes(Missing, MAE, fill = Censoring, alpha = Missing)) +\n    facet_nested( ~ Model + Censoring, switch = \"x\",\n                  strip = strip_split(position = c(\"top\", \"bottom\"))) +\n    geom_bar(stat = \"identity\") +\n    labs(fill = \"Method\") +\n    theme_classic() +\n    scale_fill_manual(values = ct_colors) +\n    scale_alpha_ordinal(range = c(1, 0.4)) +\n    theme(strip.placement = \"outside\",\n          strip.background = element_blank(),\n          axis.title.x = element_blank(),\n          legend.position = \"none\",\n          strip.text.x.top = element_text(face = \"bold\"))\n```\n\n::: {.cell-output-display}\n![MAE for Upper Censoring](index_files/figure-pdf/fig-upper-MAE-1.pdf){#fig-upper-MAE fig-pos='H'}\n:::\n:::\n\n::: {#cell-fig-upper-R .cell apa-note='For each model and each level of missingness, the Pearson correlation (R) is plotted. Higher R indicates better parameter recovery.'}\n\n```{.r .cell-code .hidden}\nEMCs_upper %>%\n  ggplot(aes(Missing, R, color = Censoring)) +\n  facet_grid( ~ Model) +\n  geom_point() +\n  geom_line(aes(group = Censoring)) + \n  labs(fill = \"Method\") +\n  theme_classic() +\n  scale_fill_manual(values = ct_colors) +\n  theme(strip.placement = \"outside\",\n        strip.background = element_blank(),\n        axis.title.x = element_blank(),\n        legend.position = \"none\",\n        strip.text.x.top = element_text(face = \"bold\"))\n```\n\n::: {.cell-output-display}\n![Pearson Correlations for Upper Censoring](index_files/figure-pdf/fig-upper-R-1.pdf){#fig-upper-R fig-pos='H'}\n:::\n:::\n\n\n\n\n## Study 2\n### Linear Ballistic Accumulator Model\n#### Model Distances\n\n\n\n::: {#cell-fig-MAE-both-LBA .cell apa-note='Mean absolute errors over all parameters for the linear ballistic accumulator model (LBA). Colors denote whether missing RTs were censored or truncated. Points were randomly horizontally shifted to avoid overlap. Lower MAE indicates better parameter recovery.'}\n\n```{.r .cell-code .hidden}\nEMCs_both_MAEs %>%\n  filter(Model == \"LBA\") %>%\n  ggplot(aes(Missing, MAE, color = Censoring, group = Censoring)) +\n  facet_grid(Response ~ Tail,\n             labeller = label_both) +\n  geom_point(position = position_jitter(width = 0.3, height = 0),\n             alpha = 0.7) +\n  labs(fill = \"Missing\") +\n  theme_classic() +\n  scale_y_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![MAEs for the LBA](index_files/figure-pdf/fig-MAE-both-LBA-1.pdf){#fig-MAE-both-LBA fig-pos='H'}\n:::\n:::\n\n::: {#cell-fig-R-both-LBA .cell apa-note='Pearson correlations (R) between the true linear ballistic accumulator model (LBA) parameters and the posterior medians. Colors denote whether missing RTs were censored or truncated. Points were randomly horizontally shifted to avoid overlap. High correlation indicates better parameter recovery.'}\n\n```{.r .cell-code .hidden}\nEMCs_both_Rs %>%\n  filter(Model == \"LBA\") %>%\n  ggplot(aes(Missing, R, color = Censoring, group = Censoring)) +\n  facet_grid(Response ~ Tail,\n             labeller = label_both) +\n  geom_point(position = position_jitter(width = 0.3, height = 0),\n             alpha = 0.7) +\n  labs(fill = \"Missing\") +\n  theme_classic() +\n  scale_y_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Pearson correlations for the LBA](index_files/figure-pdf/fig-R-both-LBA-1.pdf){#fig-R-both-LBA fig-pos='H'}\n:::\n:::\n\n\n\n\n#### Credible Intervals\n\n##### Upper Tail\n\n\n\n\n::: {#cell-fig-LBA-upper-known .cell}\n\n```{.r .cell-code .hidden}\npars_EMCs_both$id <- as.factor(pars_EMCs_both$id)\npars_EMCs_both %>%\n  filter(Model == \"LBA\" & Tail == \"upper\" & Response == \"known\") %>%\n  ggplot(aes(Median, id, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  facet_nested(Censoring + Missing ~ Variable, scales = \"free_x\", \n             labeller = labeller(.rows = label_value,\n                                 Variable = parlabeller, \n                                 .default = label_parsed)) +\n  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5)) +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() + \n  xlab(\"Parameter Estimates (95% Credible Interval)\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        strip.text.x = element_text(size = 12),\n        legend.position = \"none\") +\n  scale_color_manual(values = ct_colors) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Linear Ballistic Accumulator Posterior Medians and 95% Equal Tailed Credible Intervals with Missing Upper Tail and Responses Known](index_files/figure-pdf/fig-LBA-upper-known-1.pdf){#fig-LBA-upper-known fig-pos='H'}\n:::\n:::\n\n::: {#cell-fig-LBA-upper-unknown .cell}\n\n```{.r .cell-code .hidden}\npars_EMCs_both$id <- as.factor(pars_EMCs_both$id)\npars_EMCs_both %>%\n  filter(Model == \"LBA\" & Tail == \"upper\" & Response == \"unknown\") %>%\n  ggplot(aes(Median, id, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  facet_nested(Censoring + Missing ~ Variable, scales = \"free_x\", \n             labeller = labeller(.rows = label_value,\n                                 Variable = parlabeller, \n                                 .default = label_parsed)) +\n  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5)) +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() + \n  xlab(\"Parameter Estimates (95% Credible Interval)\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        strip.text.x = element_text(size = 12),\n        legend.position = \"none\") +\n  scale_color_manual(values = ct_colors) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Linear Ballistic Accumulator Posterior Medians and 95% Equal Tailed Credible Intervals with Missing Upper Tail and Responses Unknown](index_files/figure-pdf/fig-LBA-upper-unknown-1.pdf){#fig-LBA-upper-unknown fig-pos='H'}\n:::\n:::\n\n\n\n\n##### Lower Tail\n\n\n\n\n::: {#cell-fig-LBA-lower-known .cell}\n\n```{.r .cell-code .hidden}\npars_EMCs_both$id <- as.factor(pars_EMCs_both$id)\npars_EMCs_both %>%\n  filter(Model == \"LBA\" & Tail == \"lower\" & Response == \"known\") %>%\n  ggplot(aes(Median, id, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  facet_nested(Censoring + Missing ~ Variable, scales = \"free_x\", \n             labeller = labeller(.rows = label_value,\n                                 Variable = parlabeller, \n                                 .default = label_parsed)) +\n  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5)) +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() + \n  xlab(\"Parameter Estimates (95% Credible Interval)\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        strip.text.x = element_text(size = 12),\n        legend.position = \"none\") +\n  scale_color_manual(values = ct_colors) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Linear Ballistic Accumulator Posterior Medians and 95% Equal Tailed Credible Intervals with Missing Lower Tail and Responses Known](index_files/figure-pdf/fig-LBA-lower-known-1.pdf){#fig-LBA-lower-known fig-pos='H'}\n:::\n:::\n\n::: {#cell-fig-LBA-lower-unknown .cell}\n\n```{.r .cell-code .hidden}\npars_EMCs_both$id <- as.factor(pars_EMCs_both$id)\npars_EMCs_both %>%\n  filter(Model == \"LBA\" & Tail == \"lower\" & Response == \"unknown\") %>%\n  ggplot(aes(Median, id, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  facet_nested(Censoring + Missing ~ Variable, scales = \"free_x\", \n             labeller = labeller(.rows = label_value,\n                                 Variable = parlabeller, \n                                 .default = label_parsed)) +\n  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5)) +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() + \n  xlab(\"Parameter Estimates (95% Credible Interval)\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        strip.text.x = element_text(size = 12),\n        legend.position = \"none\") +\n  scale_color_manual(values = ct_colors) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Linear Ballistic Accumulator Posterior Medians and 95% Equal Tailed Credible Intervals with Missing Lower Tail and Responses Unknown](index_files/figure-pdf/fig-LBA-lower-unknown-1.pdf){#fig-LBA-lower-unknown fig-pos='H'}\n:::\n:::\n\n\n\n\n##### Both Tails\n\n\n\n\n::: {#cell-fig-LBA-both-known .cell}\n\n```{.r .cell-code .hidden}\npars_EMCs_both$id <- as.factor(pars_EMCs_both$id)\npars_EMCs_both %>%\n  filter(Model == \"LBA\" & Tail == \"both\" & Response == \"known\") %>%\n  ggplot(aes(Median, id, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  facet_nested(Censoring + Missing ~ Variable, scales = \"free_x\", \n             labeller = labeller(.rows = label_value,\n                                 Variable = parlabeller, \n                                 .default = label_parsed)) +\n  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5)) +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() + \n  xlab(\"Parameter Estimates (95% Credible Interval)\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        strip.text.x = element_text(size = 12),\n        legend.position = \"none\") +\n  scale_color_manual(values = ct_colors) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Linear Ballistic Accumulator Posterior Medians and 95% Equal Tailed Credible Intervals with Missing Upper and Lower Tail and Responses Known](index_files/figure-pdf/fig-LBA-both-known-1.pdf){#fig-LBA-both-known fig-pos='H'}\n:::\n:::\n\n::: {#cell-fig-LBA-both-unknown .cell}\n\n```{.r .cell-code .hidden}\npars_EMCs_both$id <- as.factor(pars_EMCs_both$id)\npars_EMCs_both %>%\n  filter(Model == \"LBA\" & Tail == \"both\" & Response == \"unknown\") %>%\n  ggplot(aes(Median, id, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  facet_nested(Censoring + Missing ~ Variable, scales = \"free_x\", \n             labeller = labeller(.rows = label_value,\n                                 Variable = parlabeller, \n                                 .default = label_parsed)) +\n  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5)) +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() + \n  xlab(\"Parameter Estimates (95% Credible Interval)\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        strip.text.x = element_text(size = 12),\n        legend.position = \"none\") +\n  scale_color_manual(values = ct_colors) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Linear Ballistic Accumulator Posterior Medians and 95% Equal Tailed Credible Intervals with Missing Upper and Lower Tail and Responses Unknown](index_files/figure-pdf/fig-LBA-both-unknown-1.pdf){#fig-LBA-both-unknown fig-pos='H'}\n:::\n:::\n\n\n\n\n### Log-Normal Race Model\n#### Model Distances\n\n\n\n::: {#cell-fig-MAE-both-LNR .cell apa-note='Mean absolute errors over all parameters for the log-normal race model (LNR). Colors denote whether missing RTs were censored or truncated. Points were randomly horizontally shifted to avoid overlap. Lower MAE indicates better parameter recovery.'}\n\n```{.r .cell-code .hidden}\nEMCs_both_MAEs %>%\n  filter(Model == \"LNR\") %>%\n  ggplot(aes(Missing, MAE, color = Censoring, group = Censoring)) +\n  facet_grid(Response ~ Tail,\n             labeller = label_both) +\n  geom_point(position = position_jitter(width = 0.3, height = 0),\n             alpha = 0.7) +\n  labs(fill = \"Missing\") +\n  theme_classic() +\n  scale_y_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![MAEs for the LNR](index_files/figure-pdf/fig-MAE-both-LNR-1.pdf){#fig-MAE-both-LNR fig-pos='H'}\n:::\n:::\n\n::: {#cell-fig-R-both-LNR .cell apa-note='Pearson correlations (R) between the true log-normal race model (LNR) parameters and the posterior medians. Colors denote whether missing RTs were censored or truncated. Points were randomly horizontally shifted to avoid overlap. High correlation indicates better parameter recovery.'}\n\n```{.r .cell-code .hidden}\nEMCs_both_Rs %>%\n  filter(Model == \"LNR\") %>%\n  ggplot(aes(Missing, R, color = Censoring, group = Censoring)) +\n  facet_grid(Response ~ Tail,\n             labeller = label_both) +\n  geom_point(position = position_jitter(width = 0.3, height = 0),\n             alpha = 0.7) +\n  labs(fill = \"Missing\") +\n  theme_classic() +\n  scale_y_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Pearson correlations for the LNR](index_files/figure-pdf/fig-R-both-LNR-1.pdf){#fig-R-both-LNR fig-pos='H'}\n:::\n:::\n\n\n\n\n#### Credible Intervals\n\n##### Upper Tail\n\n\n\n\n::: {#cell-fig-LNR-upper-known .cell}\n\n```{.r .cell-code .hidden}\npars_EMCs_both$id <- as.factor(pars_EMCs_both$id)\npars_EMCs_both %>%\n  filter(Model == \"LNR\" & Tail == \"upper\" & Response == \"known\") %>%\n  ggplot(aes(Median, id, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  facet_nested(Censoring + Missing ~ Variable, scales = \"free_x\", \n             labeller = labeller(.rows = label_value,\n                                 Variable = parlabeller, \n                                 .default = label_parsed)) +\n  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5)) +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() + \n  xlab(\"Parameter Estimates (95% Credible Interval)\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        strip.text.x = element_text(size = 12),\n        legend.position = \"none\") +\n  scale_color_manual(values = ct_colors) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Log-Normal Race Posterior Medians and 95% Equal Tailed Credible Intervals with Missing Upper Tail and Responses Known](index_files/figure-pdf/fig-LNR-upper-known-1.pdf){#fig-LNR-upper-known fig-pos='H'}\n:::\n:::\n\n::: {#cell-fig-LNR-upper-unknown .cell}\n\n```{.r .cell-code .hidden}\npars_EMCs_both$id <- as.factor(pars_EMCs_both$id)\npars_EMCs_both %>%\n  filter(Model == \"LNR\" & Tail == \"upper\" & Response == \"unknown\") %>%\n  ggplot(aes(Median, id, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  facet_nested(Censoring + Missing ~ Variable, scales = \"free_x\", \n             labeller = labeller(.rows = label_value,\n                                 Variable = parlabeller, \n                                 .default = label_parsed)) +\n  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5)) +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() + \n  xlab(\"Parameter Estimates (95% Credible Interval)\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        strip.text.x = element_text(size = 12),\n        legend.position = \"none\") +\n  scale_color_manual(values = ct_colors) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Log-Normal Race Posterior Medians and 95% Equal Tailed Credible Intervals with Missing Upper Tail and Responses Unknown](index_files/figure-pdf/fig-LNR-upper-unknown-1.pdf){#fig-LNR-upper-unknown fig-pos='H'}\n:::\n:::\n\n\n\n\n##### Lower Tail\n\n\n\n\n::: {#cell-fig-LNR-lower-known .cell}\n\n```{.r .cell-code .hidden}\npars_EMCs_both$id <- as.factor(pars_EMCs_both$id)\npars_EMCs_both %>%\n  filter(Model == \"LNR\" & Tail == \"lower\" & Response == \"known\") %>%\n  ggplot(aes(Median, id, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  facet_nested(Censoring + Missing ~ Variable, scales = \"free_x\", \n             labeller = labeller(.rows = label_value,\n                                 Variable = parlabeller, \n                                 .default = label_parsed)) +\n  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5)) +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() + \n  xlab(\"Parameter Estimates (95% Credible Interval)\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        strip.text.x = element_text(size = 12),\n        legend.position = \"none\") +\n  scale_color_manual(values = ct_colors) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Log-Normal Race Posterior Medians and 95% Equal Tailed Credible Intervals with Missing Lower Tail and Responses Known](index_files/figure-pdf/fig-LNR-lower-known-1.pdf){#fig-LNR-lower-known fig-pos='H'}\n:::\n:::\n\n::: {#cell-fig-LNR-lower-unknown .cell}\n\n```{.r .cell-code .hidden}\npars_EMCs_both$id <- as.factor(pars_EMCs_both$id)\npars_EMCs_both %>%\n  filter(Model == \"LNR\" & Tail == \"lower\" & Response == \"unknown\") %>%\n  ggplot(aes(Median, id, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  facet_nested(Censoring + Missing ~ Variable, scales = \"free_x\", \n             labeller = labeller(.rows = label_value,\n                                 Variable = parlabeller, \n                                 .default = label_parsed)) +\n  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5)) +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() + \n  xlab(\"Parameter Estimates (95% Credible Interval)\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        strip.text.x = element_text(size = 12),\n        legend.position = \"none\") +\n  scale_color_manual(values = ct_colors) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Log-Normal Race Posterior Medians and 95% Equal Tailed Credible Intervals with Missing Lower Tail and Responses Unknown](index_files/figure-pdf/fig-LNR-lower-unknown-1.pdf){#fig-LNR-lower-unknown fig-pos='H'}\n:::\n:::\n\n\n\n\n##### Both Tails\n\n\n\n\n::: {#cell-fig-LNR-both-known .cell}\n\n```{.r .cell-code .hidden}\npars_EMCs_both$id <- as.factor(pars_EMCs_both$id)\npars_EMCs_both %>%\n  filter(Model == \"LNR\" & Tail == \"both\" & Response == \"known\") %>%\n  ggplot(aes(Median, id, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  facet_nested(Censoring + Missing ~ Variable, scales = \"free_x\", \n             labeller = labeller(.rows = label_value,\n                                 Variable = parlabeller, \n                                 .default = label_parsed)) +\n  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5)) +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() + \n  xlab(\"Parameter Estimates (95% Credible Interval)\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        strip.text.x = element_text(size = 12),\n        legend.position = \"none\") +\n  scale_color_manual(values = ct_colors) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Log-Normal Race Posterior Medians and 95% Equal Tailed Credible Intervals with Missing Upper and Lower Tail and Responses Known](index_files/figure-pdf/fig-LNR-both-known-1.pdf){#fig-LNR-both-known fig-pos='H'}\n:::\n:::\n\n::: {#cell-fig-LNR-both-unknown .cell}\n\n```{.r .cell-code .hidden}\npars_EMCs_both$id <- as.factor(pars_EMCs_both$id)\npars_EMCs_both %>%\n  filter(Model == \"LNR\" & Tail == \"both\" & Response == \"unknown\") %>%\n  ggplot(aes(Median, id, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  facet_nested(Censoring + Missing ~ Variable, scales = \"free_x\", \n             labeller = labeller(.rows = label_value,\n                                 Variable = parlabeller, \n                                 .default = label_parsed)) +\n  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5)) +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() + \n  xlab(\"Parameter Estimates (95% Credible Interval)\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        strip.text.x = element_text(size = 12),\n        legend.position = \"none\") +\n  scale_color_manual(values = ct_colors) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Log-Normal Race Posterior Medians and 95% Equal Tailed Credible Intervals with Missing Upper and Lower Tail and Responses Unknown](index_files/figure-pdf/fig-LNR-both-unknown-1.pdf){#fig-LNR-both-unknown fig-pos='H'}\n:::\n:::\n\n\n\n\n### Racing Diffusion Model\n#### Model Distances\n\n\n\n::: {#cell-fig-MAE-both-RDM .cell apa-note='Mean absolute errors over all parameters for the racing diffusion model (RDM). Colors denote whether missing RTs were censored or truncated. Points were randomly horizontally shifted to avoid overlap. Lower MAE indicates better parameter recovery.'}\n\n```{.r .cell-code .hidden}\nEMCs_both_MAEs %>%\n  filter(Model == \"RDM\") %>%\n  ggplot(aes(Missing, MAE, color = Censoring, group = Censoring)) +\n  facet_grid(Response ~ Tail,\n             labeller = label_both) +\n  geom_point(position = position_jitter(width = 0.3, height = 0),\n             alpha = 0.7) +\n  labs(fill = \"Missing\") +\n  theme_classic() +\n  scale_y_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![MAEs for the RDM](index_files/figure-pdf/fig-MAE-both-RDM-1.pdf){#fig-MAE-both-RDM fig-pos='H'}\n:::\n:::\n\n::: {#cell-fig-R-both-RDM .cell apa-note='Pearson correlations (R) between the true racing diffusion model (RDM) parameters and the posterior medians. Colors denote whether missing RTs were censored or truncated. Points were randomly horizontally shifted to avoid overlap. High correlation indicates better parameter recovery.'}\n\n```{.r .cell-code .hidden}\nEMCs_both_Rs %>%\n  filter(Model == \"RDM\") %>%\n  ggplot(aes(Missing, R, color = Censoring, group = Censoring)) +\n  facet_grid(Response ~ Tail,\n             labeller = label_both) +\n  geom_point(position = position_jitter(width = 0.3, height = 0),\n             alpha = 0.7) +\n  labs(fill = \"Missing\") +\n  theme_classic() +\n  scale_y_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Pearson correlations for the RDM](index_files/figure-pdf/fig-R-both-RDM-1.pdf){#fig-R-both-RDM fig-pos='H'}\n:::\n:::\n\n\n\n\n#### Credible Intervals\n\n##### Upper Tail\n\n\n\n\n::: {#cell-fig-RDM-upper-known .cell}\n\n```{.r .cell-code .hidden}\npars_EMCs_both$id <- as.factor(pars_EMCs_both$id)\npars_EMCs_both %>%\n  filter(Model == \"RDM\" & Tail == \"upper\" & Response == \"known\") %>%\n  ggplot(aes(Median, id, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  facet_nested(Censoring + Missing ~ Variable, scales = \"free_x\", \n             labeller = labeller(.rows = label_value,\n                                 Variable = parlabeller, \n                                 .default = label_parsed)) +\n  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5)) +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() + \n  xlab(\"Parameter Estimates (95% Credible Interval)\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        strip.text.x = element_text(size = 12),\n        legend.position = \"none\") +\n  scale_color_manual(values = ct_colors) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Racing Diffusion Model Posterior Medians and 95% Equal Tailed Credible Intervals with Missing Upper Tail and Responses Known](index_files/figure-pdf/fig-RDM-upper-known-1.pdf){#fig-RDM-upper-known fig-pos='H'}\n:::\n:::\n\n::: {#cell-fig-RDM-upper-unknown .cell}\n\n```{.r .cell-code .hidden}\npars_EMCs_both$id <- as.factor(pars_EMCs_both$id)\npars_EMCs_both %>%\n  filter(Model == \"RDM\" & Tail == \"upper\" & Response == \"unknown\") %>%\n  ggplot(aes(Median, id, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  facet_nested(Censoring + Missing ~ Variable, scales = \"free_x\", \n             labeller = labeller(.rows = label_value,\n                                 Variable = parlabeller, \n                                 .default = label_parsed)) +\n  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5)) +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() + \n  xlab(\"Parameter Estimates (95% Credible Interval)\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        strip.text.x = element_text(size = 12),\n        legend.position = \"none\") +\n  scale_color_manual(values = ct_colors) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Racing Diffusion Model Posterior Medians and 95% Equal Tailed Credible Intervals with Missing Upper Tail and Responses Unknown](index_files/figure-pdf/fig-RDM-upper-unknown-1.pdf){#fig-RDM-upper-unknown fig-pos='H'}\n:::\n:::\n\n\n\n\n##### Lower Tail\n\n\n\n\n::: {#cell-fig-RDM-lower-known .cell}\n\n```{.r .cell-code .hidden}\npars_EMCs_both$id <- as.factor(pars_EMCs_both$id)\npars_EMCs_both %>%\n  filter(Model == \"RDM\" & Tail == \"lower\" & Response == \"known\") %>%\n  ggplot(aes(Median, id, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  facet_nested(Censoring + Missing ~ Variable, scales = \"free_x\", \n             labeller = labeller(.rows = label_value,\n                                 Variable = parlabeller, \n                                 .default = label_parsed)) +\n  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5)) +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() + \n  xlab(\"Parameter Estimates (95% Credible Interval)\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        strip.text.x = element_text(size = 12),\n        legend.position = \"none\") +\n  scale_color_manual(values = ct_colors) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Racing Diffusion Model Posterior Medians and 95% Equal Tailed Credible Intervals with Missing Lower Tail and Responses Known](index_files/figure-pdf/fig-RDM-lower-known-1.pdf){#fig-RDM-lower-known fig-pos='H'}\n:::\n:::\n\n::: {#cell-fig-RDM-lower-unknown .cell}\n\n```{.r .cell-code .hidden}\npars_EMCs_both$id <- as.factor(pars_EMCs_both$id)\npars_EMCs_both %>%\n  filter(Model == \"RDM\" & Tail == \"lower\" & Response == \"unknown\") %>%\n  ggplot(aes(Median, id, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  facet_nested(Censoring + Missing ~ Variable, scales = \"free_x\", \n             labeller = labeller(.rows = label_value,\n                                 Variable = parlabeller, \n                                 .default = label_parsed)) +\n  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5)) +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() + \n  xlab(\"Parameter Estimates (95% Credible Interval)\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        strip.text.x = element_text(size = 12),\n        legend.position = \"none\") +\n  scale_color_manual(values = ct_colors) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Racing Diffusion Model Posterior Medians and 95% Equal Tailed Credible Intervals with Missing Lower Tail and Responses Unknown](index_files/figure-pdf/fig-RDM-lower-unknown-1.pdf){#fig-RDM-lower-unknown fig-pos='H'}\n:::\n:::\n\n\n\n\n##### Both Tails\n\n\n\n\n::: {#cell-fig-RDM-both-known .cell}\n\n```{.r .cell-code .hidden}\npars_EMCs_both$id <- as.factor(pars_EMCs_both$id)\npars_EMCs_both %>%\n  filter(Model == \"RDM\" & Tail == \"both\" & Response == \"known\") %>%\n  ggplot(aes(Median, id, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  facet_nested(Censoring + Missing ~ Variable, scales = \"free_x\", \n             labeller = labeller(.rows = label_value,\n                                 Variable = parlabeller, \n                                 .default = label_parsed)) +\n  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5)) +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() + \n  xlab(\"Parameter Estimates (95% Credible Interval)\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        strip.text.x = element_text(size = 12),\n        legend.position = \"none\") +\n  scale_color_manual(values = ct_colors) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Racing Diffusion Model Posterior Medians and 95% Equal Tailed Credible Intervals with Missing Upper and Lower Tail and Responses Known](index_files/figure-pdf/fig-RDM-both-known-1.pdf){#fig-RDM-both-known fig-pos='H'}\n:::\n:::\n\n::: {#cell-fig-RDM-both-unknown .cell}\n\n```{.r .cell-code .hidden}\npars_EMCs_both$id <- as.factor(pars_EMCs_both$id)\npars_EMCs_both %>%\n  filter(Model == \"RDM\" & Tail == \"both\" & Response == \"unknown\") %>%\n  ggplot(aes(Median, id, color = Censoring, alpha = Missing)) +\n  geom_point() +\n  facet_nested(Censoring + Missing ~ Variable, scales = \"free_x\", \n             labeller = labeller(.rows = label_value, \n                                 Variable = parlabeller, \n                                 .default = label_parsed)) +\n  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5)) +\n  geom_vline(aes(xintercept = True), linetype = \"dashed\") +\n  theme_classic() + \n  xlab(\"Parameter Estimates (95% Credible Interval)\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        strip.text.x = element_text(size = 12),\n        legend.position = \"none\") +\n  scale_color_manual(values = ct_colors) +\n  scale_alpha_ordinal(range = c(1, 0.4)) +\n  scale_x_continuous(n.breaks = 3)\n```\n\n::: {.cell-output-display}\n![Racing Diffusion Model Posterior Medians and 95% Equal Tailed Credible Intervals with Missing Upper and Lower Tail and Responses Unknown](index_files/figure-pdf/fig-RDM-both-unknown-1.pdf){#fig-RDM-both-unknown fig-pos='H'}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}